{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Logistic Regression Assignment Questions\n",
        "\n",
        "#Theoretical\n"
      ],
      "metadata": {
        "id": "9fd6O3p-d3HW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is Logistic Regression, and how does it differ from Linear Regression ?\n",
        "\n",
        "Ans-1. **Logistic Regression**\n",
        "\n",
        "* **What it is:** Logistic regression is a statistical method used for binary classification problems. This means it predicts the probability of an outcome that can only have two values (e.g., yes/no, true/false, 0/1).\n",
        "\n",
        "* **How it works:** It uses a logistic function (also known as a sigmoid function) to transform a linear combination of input features into a probability between 0 and 1. This probability is then used to classify the data point into one of the two categories.\n",
        "\n",
        "####Key points:\n",
        "\n",
        "* **Output:** Predicts probabilities.\n",
        "\n",
        "* **Use case:** Classification problems.\n",
        "\n",
        "* **Logistic function:** Transforms the input into a probability.\n",
        "\n",
        "**Linear Regression**\n",
        "\n",
        "* **What it is:** Linear regression is a statistical method used to model the linear relationship between a dependent variable and one or more independent variables.\n",
        "\n",
        "* **How it works:** It finds the best-fitting line that represents the relationship between the variables, allowing you to predict the value of the dependent variable based on the independent variables.\n",
        "\n",
        "####Key points:\n",
        "\n",
        "* **Output:** Predicts continuous values.\n",
        "\n",
        "* **Use case:** Regression problems (predicting a quantity).\n",
        "\n",
        "* **Linear relationship:** Assumes a straight-line relationship between variables.\n",
        "\n",
        "**Analogy**\n",
        "\n",
        "* Imagine you're trying to predict:\n",
        "\n",
        "* **Logistic Regression:** Whether a student will pass or fail an exam (two categories).\n",
        "\n",
        "* **Linear Regression:** A student's score on an exam (a continuous value).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EXwvsLiId8V3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is the mathematical equation of Logistic Regression ?\n",
        "\n",
        "Ans-2. The mathematical equation for logistic regression can be expressed in a few ways, but the most common and fundamental form is:\n",
        "\n",
        "    p = 1 / (1 + exp(-z))\n",
        "\n",
        "####**Where:**\n",
        "\n",
        "* **p:** is the probability of the event occurring (the output of the logistic  regression). It's a value between 0 and 1.\n",
        "\n",
        "* **exp():** is the exponential function (e raised to the power of...).\n",
        "\n",
        "* **z:** is a linear combination of the input features. It's calculated as:\n",
        "    \n",
        "      z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
        "\n",
        "####**Where:**\n",
        "\n",
        "* β₀: is the intercept (the value of z when all x's are zero).\n",
        "\n",
        "* β₁, β₂, ..., βₙ: are the coefficients (weights) of the input features. These are what the model learns during training.\n",
        "\n",
        "*x₁, x₂, ..., xₙ: are the input features (independent variables).\n",
        "\n",
        "####**Putting it all together:**\n",
        "\n",
        "* You can substitute the equation for z into the equation for p:\n",
        "\n",
        "      p = 1 / (1 + exp(-(β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ)))\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "* The logistic regression equation takes your input features (x₁, x₂, etc.), multiplies them by learned weights (β₁, β₂, etc.), adds them up (along with an intercept β₀), and then plugs that result into a special function (the sigmoid function, 1 / (1 + exp(-z))) to get a probability (p) between 0 and 1.  This probability represents the likelihood of the event you're trying to predict."
      ],
      "metadata": {
        "id": "oDB0o7Swo9gA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Why do we use the Sigmoid function in Logistic Regression ?\n",
        "\n",
        "Ans-3. why the sigmoid function is so important in logistic regression! It's not just some random curve; it plays a crucial role.\n",
        "\n",
        "####**Here's why:**\n",
        "\n",
        "1. **Probability Interpretation:**\n",
        "\n",
        "* **The Goal:** In logistic regression, we want to predict probabilities (between 0 and 1). For example, the probability that an email is spam, or the probability that a customer will click on an ad.\n",
        "\n",
        "* **Sigmoid's Magic:** The sigmoid function takes any real number as input (which is what the linear combination of features gives us) and squishes it into a value between 0 and 1. This perfectly matches the range of probabilities.\n",
        "\n",
        "2. **Classification:**\n",
        "\n",
        "* **Decision Boundary:** Once we have a probability from the sigmoid, we can easily make a decision. We typically set a threshold (often 0.5). If the probability is above the threshold, we classify the data point into one category; otherwise, into the other.\n",
        "\n",
        "* **Smooth Transition:** The sigmoid provides a smooth transition between categories. It doesn't give a hard \"yes\" or \"no\" but rather a probability, which is more nuanced and realistic.\n",
        "\n",
        "3. **Mathematical Convenience:**\n",
        "\n",
        "* **Differentiable:** The sigmoid function is differentiable, which is essential for the optimization algorithms (like gradient descent) used to train the logistic regression model. These algorithms need to calculate the slope of the function to find the best parameters.\n",
        "\n",
        "* **Well-behaved:** The sigmoid function has nice mathematical properties that make it easier to work with in the context of logistic regression.\n",
        "\n",
        "####In summary:\n",
        "\n",
        "The sigmoid function is the key to logistic regression because it:\n",
        "\n",
        "* Transforms the output of a linear combination of features into a probability.\n",
        "\n",
        "* Allows for easy classification by setting a threshold.\n",
        "\n",
        "* Has desirable mathematical properties for training the model.\n",
        "\n",
        "Without the sigmoid function, logistic regression wouldn't be able to do what it does!"
      ],
      "metadata": {
        "id": "b0RcaUANr0vM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. What is the cost function of Logistic Regression ?\n",
        "\n",
        "Ans-4. The cost function in logistic regression measures how well the model is performing.  It quantifies the \"error\" between the predicted probabilities and the actual outcomes.  The goal during training is to minimize this cost function.\n",
        "\n",
        "The most common cost function used in logistic regression is the log loss (also known as binary cross-entropy).  \n",
        "\n",
        "* **Here's the formula:**\n",
        "\n",
        "      J(θ) = -1/m * Σ [yᵢ * log(h(xᵢ)) + (1 - yᵢ) * log(1 - h(xᵢ))]\n",
        "\n",
        "####**Where:**\n",
        "\n",
        "* J(θ): is the cost function.\n",
        "\n",
        "* m: is the number of training examples.\n",
        "\n",
        "* yᵢ: is the actual label of the i-th training example (0 or 1).\n",
        "\n",
        "* h(xᵢ): is the predicted probability of the i-th training example, calculated using the sigmoid function:\n",
        "   \n",
        "               h(xᵢ) = 1 / (1 + exp(-θᵀxᵢ))\n",
        "\n",
        "* θ: is the vector of model parameters (coefficients).\n",
        "\n",
        "* xᵢ: is the vector of features for the i-th training example.\n",
        "\n",
        "* Σ: represents the sum over all training examples (from i=1 to m).\n",
        "\n",
        "\n",
        "####**Why this cost function?**\n",
        "\n",
        "The log loss has some nice properties that make it suitable for logistic regression:\n",
        "\n",
        "* **Convexity:** The log loss function is convex, meaning it has a single minimum. This is important because it guarantees that optimization algorithms (like gradient descent) will find the global minimum and not get stuck in local minima.\n",
        "\n",
        "* **Intuitive Interpretation:** The cost function penalizes incorrect predictions more heavily. If the true label is 1 and the predicted probability is close to 0, the cost will be very high. Conversely, if the true label is 1 and the predicted probability is close to 1, the cost will be close to 0.\n",
        "Differentiability: The log loss function is differentiable, which is crucial for gradient-based optimization methods.\n",
        "\n",
        "####**In simpler terms:**\n",
        "\n",
        "* The cost function basically checks how far off our predictions are from the actual values. It adds up the \"errors\" for each training example.  The bigger the errors, the higher the cost.  Our goal is to find the parameters (θ) that make this cost as low as possible.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BD7HBH9ns9BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. What is Regularization in Logistic Regression? Why is it needed ?\n",
        "\n",
        "Ans-5. Regularization in logistic regression is a technique to prevent overfitting, which is a common problem. Here's the breakdown:\n",
        "\n",
        "####**What is Overfitting?**\n",
        "\n",
        "Imagine you're training a model to predict whether a movie will be a hit or a flop. If your model is too complex, it might learn the training data too well, including all the noise and specific quirks of that particular dataset. This leads to a model that performs great on the training data but fails miserably on new, unseen movies. That's overfitting!\n",
        "\n",
        "\n",
        "####**Why Regularization?**\n",
        "\n",
        "Regularization is like a \"penalty\" for overly complex models.It discourages the model from learning very large coefficients (weights) for the features. By keeping the coefficients smaller, the model becomes simpler and less prone to overfitting. It's like telling the model, \"Don't get too carried away with the details; focus on the bigger picture.\"\n",
        "\n",
        "**How it Works in Logistic Regression**\n",
        "\n",
        "There are two main types of regularization used in logistic regression:\n",
        "\n",
        "* **L1 Regularization (Lasso):** This adds a penalty proportional to the absolute value of the coefficients. It can actually shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        "* **L2 Regularization (Ridge):** This adds a penalty proportional to the square of the coefficients. It shrinks the coefficients towards zero but usually doesn't eliminate them completely.\n",
        "\n",
        "####**Benefits of Regularization**\n",
        "\n",
        "* **Improved Generalization:** The model performs better on new, unseen data.\n",
        "\n",
        "* **Reduced Overfitting:** The model is less likely to memorize the training data.\n",
        "\n",
        "* **Feature Selection (L1):** Can help identify the most important features.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "Regularization is like a \"brake\" that prevents the model from becoming too complex and overspecialized. It encourages the model to find a simpler, more generalizable solution that works well on a variety of data.\n",
        "\n",
        "####**Analogy:**\n",
        "\n",
        "Think of studying for an exam. Overfitting is like memorizing every single detail in the textbook. Regularization is like understanding the key concepts and relationships, so you can apply your knowledge to different types of questions."
      ],
      "metadata": {
        "id": "mb1hor9yu3ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "Ans-6. Lasso, Ridge, and Elastic Net regression are all techniques used in linear regression for regularization, which means they add a penalty to the loss function to prevent overfitting and improve the model's generalizability.\n",
        "\n",
        "####**Here's a brief comparison of each:**\n",
        "\n",
        "##1. **Lasso Regression** (Least Absolute Shrinkage and Selection Operator)\n",
        "\n",
        "* **Penalty:** Adds the absolute value of the coefficients (L1 penalty) to the loss function.\n",
        "\n",
        "* **Effect:** Can shrink some coefficients to exactly zero, effectively performing variable selection.\n",
        "\n",
        "* **Use case:** Useful when you have many features and want to select a sparse model with fewer features.\n",
        "\n",
        "##2. **Ridge Regression**\n",
        "\n",
        "* **Penalty:** Adds the squared value of the coefficients (L2 penalty) to the loss function.\n",
        "\n",
        "* **Effect:** Shrinks the coefficients but does not set any of them to zero.\n",
        "\n",
        "* **Use case:** Useful when all features are potentially relevant, and you want to shrink coefficients to reduce multicollinearity and overfitting.\n",
        "\n",
        "## 3. **Elastic Net Regression**\n",
        "\n",
        "* **Penalty:** Combines both L1 and L2 penalties, with an additional parameter to control the balance between them.\n",
        "\n",
        "* **Effect:** Provides a compromise between Lasso and Ridge, allowing for both feature selection and coefficient shrinkage.\n",
        "\n",
        "* **Use case:** Useful when you want the benefits of both Lasso and Ridge, particularly when you have many correlated features.\n",
        "\n",
        "In summary:\n",
        "\n",
        "Lasso: L1 penalty, feature selection.\n",
        "\n",
        "Ridge: L2 penalty, coefficient shrinkage.\n",
        "\n",
        "Elastic Net: Combination of L1 and L2 penalties, feature selection and shrinkage."
      ],
      "metadata": {
        "id": "FJRYhJo8wT_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. When should we use Elastic Net instead of Lasso or Ridge ?\n",
        "\n",
        "Ans-7. Elastic Net is a powerful regularization technique that combines the strengths of both Lasso and Ridge regression. This makes it a versatile tool, but it also means there are specific situations where it shines. Here's when\n",
        "you should consider using Elastic Net over Lasso or Ridge:\n",
        "\n",
        "1. **When you have many correlated features:**\n",
        "\n",
        "* **The Problem:** Lasso can struggle with highly correlated features. It tends to arbitrarily select one feature from a group of correlated features and discard the rest, which can lead to instability and loss of information.\n",
        "\n",
        "* **Elastic Net's Solution:** Elastic Net handles correlated features more gracefully.It tends to group correlated features together, either keeping them all or removing them all together. This provides more stability and a better representation of the underlying relationships in the data.\n",
        "\n",
        "2. **When you're not sure about feature selection:**\n",
        "\n",
        "* **The Dilemma:** Lasso is great for feature selection, but it can be too aggressive, potentially removing features that are actually important. Ridge keeps all features, which might not be ideal if you have many irrelevant features.\n",
        "\n",
        "* **Elastic Net's Approach:** Elastic Net offers a balance. It can perform feature selection like Lasso but is less likely to eliminate truly important features due to its Ridge component. This makes it a good choice when you want some feature selection but are unsure about which features are crucial.\n",
        "\n",
        "3. **When you want a more stable model:**\n",
        "\n",
        "* **The Concern:** Lasso's feature selection can sometimes be unstable, meaning that small changes in the data can lead to different features being selected.\n",
        "Elastic Net's Advantage: Elastic Net's combination of L1 and L2 penalties makes it more stable than Lasso. It's less sensitive to small changes in the data, leading to more consistent results.\n",
        "\n",
        "**In summary, consider using Elastic Net when:**\n",
        "\n",
        "* You have many correlated features.\n",
        "\n",
        "* You want some feature selection but are unsure about which features are important.\n",
        "\n",
        "* You need a more stable model that is less sensitive to small changes in the data.\n",
        "\n",
        "**Important Note:** Elastic Net has two tuning parameters (alpha and l1_ratio), which makes it slightly more complex to tune than Lasso or Ridge. However, this added complexity is often worth it for the benefits it provides in the situations described above.\n",
        "\n"
      ],
      "metadata": {
        "id": "4bmGmOYOVvNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. What is the impact of the regularization parameter (λ) in Logistic Regression ?\n",
        "\n",
        "Ans-8. The regularization parameter, often denoted as λ (lambda) or sometimes as C (where C is the inverse of lambda), plays a crucial role in controlling the strength of regularization in logistic regression. Here's a breakdown of its impact:\n",
        "\n",
        "**What does it do?**\n",
        "\n",
        "* **Controls model complexity:** λ determines the amount of \"penalty\" applied to large coefficients in the model. A higher λ means a stronger penalty, discouraging the model from having very large coefficients. This, in turn, makes the model simpler.\n",
        "\n",
        "* **Balances bias and variance:** Regularization helps find a balance between bias and variance.\n",
        "\n",
        "* **High λ:** Increases bias (the model makes stronger assumptions) and reduces variance (the model is less sensitive to fluctuations in the training data). This can lead to underfitting if λ is too high.\n",
        "Low λ: Decreases bias and increases variance. This can lead to overfitting if λ is too low.\n",
        "\n",
        "####**Impact on Coefficients**\n",
        "\n",
        "* **Larger λ:** Shrinks the coefficients towards zero. In L1 regularization (Lasso), it can even force some coefficients to become exactly zero, effectively performing feature selection.\n",
        "\n",
        "* **Smaller λ:** Allows the coefficients to be larger, giving the model more flexibility to fit the training data.\n",
        "\n",
        "####**Impact on Overfitting**\n",
        "\n",
        "* **Larger λ:** Reduces overfitting by simplifying the model and preventing it from memorizing the training data.\n",
        "\n",
        "* **Smaller λ:** Increases the risk of overfitting, as the model can become too complex and sensitive to noise in the training data.\n",
        "\n",
        "**How to choose the right λ?**\n",
        "\n",
        "* **Cross-validation:** The most common approach is to use cross-validation. This involves splitting the data into multiple folds, training the model with different λ values on some folds, and evaluating its performance on the remaining folds. This helps find the λ that generalizes best to unseen data.\n",
        "\n",
        "* **Grid search:** You can define a range of λ values and use grid search to systematically try out different combinations of λ and other hyperparameters.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "The regularization parameter λ controls the trade-off between model complexity and its ability to fit the training data.Finding the optimal λ is crucial for building a logistic regression model that generalizes well to new, unseen data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mFp0qCaDX1Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9. What are the key assumptions of Logistic Regression ?\n",
        "\n",
        "Ans-9. The assumptions! Like any statistical model, logistic regression relies on certain assumptions to produce reliable results.\n",
        "\n",
        " **Here are the key ones:**\n",
        "\n",
        " 1. **Binary Outcome:**\n",
        "\n",
        "* **The Rule:** The dependent variable must be binary, meaning it has only two possible outcomes (e.g., yes/no, 0/1, pass/fail).\n",
        "\n",
        "* **Why it matters:** Logistic regression is specifically designed for this type of outcome. If you have a dependent variable with more than two categories, you'll need to consider other techniques (like multinomial logistic regression).\n",
        "\n",
        "2. **Independence of Observations:**\n",
        "\n",
        "* **The Rule:** The observations in your data should be independent of each other. This means that one data point should not influence another.\n",
        "\n",
        "* **Why it matters:** If observations are not independent (e.g., repeated measurements on the same individual), it can violate the assumptions of the model and lead to inaccurate results.\n",
        "\n",
        "3. **No Multicollinearity:**\n",
        "\n",
        "* **The Rule:** The independent variables should not be highly correlated with each other.\n",
        "\n",
        "* **Why it matters:** Multicollinearity can make it difficult to determine the individual effect of each predictor variable on the outcome. It can also inflate the standard errors of the coefficients, making it harder to assess statistical significance.\n",
        "\n",
        "4. **Linearity of the Logit:**\n",
        "\n",
        "* **The Rule:** The independent variables should have a linear relationship with the logit of the outcome. The logit is the natural logarithm of the odds of the outcome occurring.\n",
        "\n",
        "* **Why it matters:** This assumption ensures that the logistic regression model is correctly capturing the relationship between the predictors and the outcome.\n",
        "\n",
        "5. **Sufficient Sample Size:**\n",
        "\n",
        "* **The Rule:** You need a sufficiently large sample size to ensure the stability and reliability of the model.\n",
        "\n",
        "* **Why it matters:** With a small sample size, the model may be overly sensitive to the specific data points in your sample and may not generalize well to new data. A common rule of thumb is to have at least 10-20 events (the less frequent outcome) per predictor variable.\n",
        "\n",
        "**Important Notes:**\n",
        "\n",
        "* **No normality assumption:** Unlike linear regression, logistic regression does not assume that the error terms are normally distributed.\n",
        "\n",
        "* **Checking assumptions:** It's important to check these assumptions before interpreting the results of a logistic regression model. There are various diagnostic tools and techniques available to assess these assumptions.\n",
        "\n",
        "If these assumptions are violated, the results of the logistic regression model may be unreliable. It's crucial to be aware of these assumptions and take appropriate steps to address any violations.\n",
        "\n"
      ],
      "metadata": {
        "id": "poOhwa_qZ0sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10. What are some alternatives to Logistic Regression for classification tasks ?\n",
        "\n",
        "As-10. Here are some popular and effective alternatives, each with its own strengths and weaknesses:\n",
        "\n",
        "1. **Tree-Based Methods**\n",
        "\n",
        "* **Decision Trees:** These create a set of rules based on features to classify data. They're easy to visualize and interpret but can overfit if not pruned properly.\n",
        "\n",
        "* **Random Forests:** An ensemble method that combines multiple decision trees, improving accuracy and reducing overfitting.\n",
        "\n",
        "* **Gradient Boosting Machines (GBM):** Another ensemble method that builds trees sequentially, with each tree correcting the errors of the previous ones. Popular GBM implementations include XGBoost, LightGBM, and CatBoost.\n",
        "\n",
        "2. **Support Vector Machines (SVMs)**\n",
        "\n",
        "* **The Idea:** SVMs find the optimal hyperplane that best separates data points of different classes, maximizing the margin between them.\n",
        "\n",
        "* **Kernels:** SVMs can use different kernel functions to handle non-linearly separable data by mapping it to a higher-dimensional space.\n",
        "\n",
        "* **Strengths:** Effective in high-dimensional spaces, versatile due to kernels.\n",
        "\n",
        "* **Weaknesses:** Can be computationally expensive, difficult to interpret compared to trees.\n",
        "\n",
        "3. **K-Nearest Neighbors (KNN)**\n",
        "\n",
        "* **The Idea:** Classifies a data point based on the majority class among its k-nearest neighbors in the feature space.\n",
        "\n",
        "* **Strengths:** Simple to understand and implement, no training phase.\n",
        "Weaknesses: Computationally expensive for large datasets, sensitive to irrelevant features.\n",
        "\n",
        "4. **Naive Bayes**\n",
        "\n",
        "* **The Idea:** Based on Bayes' theorem, it calculates the probability of a data point belonging to a class based on the probabilities of its features.\n",
        "\n",
        "* **Strengths:** Simple and efficient, works well with high-dimensional data.\n",
        "\n",
        "* **Weaknesses:** Assumes feature independence, which may not hold in real-world data.\n",
        "\n",
        "5. **Neural Networks**\n",
        "\n",
        "* **The Idea:** Complex models inspired by the human brain, composed of interconnected layers of nodes.\n",
        "\n",
        "* **Deep Learning:** Neural networks with multiple layers can learn complex patterns and representations from data.\n",
        "\n",
        "* **Strengths:** Can achieve high accuracy, especially with large datasets.\n",
        "\n",
        "* **Weaknesses:** Can be computationally expensive, require careful tuning, and can be a \"black box\" (hard to interpret).\n",
        "\n",
        "**Choosing the right alternative:**\n",
        "\n",
        "The best choice depends on your specific dataset, goals, and constraints:\n",
        "\n",
        "* **Interpretability:** If you need to understand why a model makes a certain prediction, decision trees or logistic regression might be preferable.\n",
        "\n",
        "* **Accuracy:** If high accuracy is the primary goal, random forests, GBMs, SVMs, or neural networks might be good options.\n",
        "\n",
        "* **Computational resources:** If you have limited computational resources, Naive Bayes or KNN might be more suitable.\n",
        "\n",
        "* **Data characteristics:** The nature of your data (e.g., number of features, presence of non-linear relationships) can also influence the choice of model.\n",
        "\n",
        "It's often a good idea to try out several different models and compare their performance using appropriate evaluation metrics (e.g., accuracy, precision, recall, F1-score) to choose the best one for your task."
      ],
      "metadata": {
        "id": "Hgo0U54cdul1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. What are Classification Evaluation Metrics ?\n",
        "\n",
        "Ans-11. They quantify how well the model is doing at predicting the correct categories. Here's a breakdown of the most common and important ones:\n",
        "\n",
        "1. **Confusion Matrix:**\n",
        "\n",
        "* **What it is:** A table that summarizes the model's predictions compared to the actual true labels.\n",
        "\n",
        "* **Why it's important:** Provides a detailed view of the model's performance, showing not just how many predictions are correct but also where it's making mistakes.\n",
        "\n",
        "                               Actual Positive       Actual Negative      \n",
        "      Predicted Positive       True Positive (TP)    False Positive (FP)\n",
        "      Predicted Negative       False Negative (FN)    True Negative (TN)\n",
        "\n",
        " 2. **Accuracy:**\n",
        "\n",
        "* **What it is:** The overall proportion of correctly classified instances.\n",
        "\n",
        "* **Formula:** Accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
        "\n",
        "* **Pros:** Easy to understand.\n",
        "\n",
        "* **Cons:** Can be misleading if the classes are imbalanced (e.g., one class has many more examples than the other).\n",
        "\n",
        "3. **Precision:**\n",
        "\n",
        "* **What it is:** The proportion of correctly predicted positive cases out of all cases predicted as positive.\n",
        "\n",
        "* **Formula:** Precision = TP / (TP + FP)\n",
        "\n",
        "* **Pros:** Useful when minimizing false positives is important (e.g., spam detection).\n",
        "\n",
        "4. **Recall (Sensitivity or True Positive Rate):**\n",
        "\n",
        "* **What it is:** The proportion of correctly predicted positive cases out of all actual positive cases.\n",
        "\n",
        "* **Formula:** Recall = TP / (TP + FN)\n",
        "\n",
        "* **Pros:** Useful when minimizing false negatives is important (e.g., disease diagnosis).\n",
        "\n",
        "5. **F1-Score:**\n",
        "\n",
        "* **What it is:** The harmonic mean of precision and recall.\n",
        "\n",
        "* **Formula:** F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "* **Pros:** Provides a balanced measure that considers both precision and recall. Useful when you want to find a good compromise between them.\n",
        "\n",
        "6. **Specificity (True Negative Rate):**\n",
        "\n",
        "* **What it is:** The proportion of correctly predicted negative cases out of all actual negative cases.\n",
        "\n",
        "* **Formula:** Specificity = TN / (TN + FP)\n",
        "\n",
        "* **Pros:** Useful when identifying true negatives is important.\n",
        "\n",
        "7. **ROC Curve (Receiver Operating Characteristic):**\n",
        "\n",
        "* **What it is:** A plot that shows the trade-off between true positive rate (recall) and false positive rate (1 - specificity) at different classification thresholds.\n",
        "\n",
        "* **Why it's important:** Helps visualize the model's performance across different thresholds and choose an appropriate threshold for your specific needs.\n",
        "\n",
        "8. **AUC (Area Under the Curve):**\n",
        "\n",
        "* **What it is:** A single value that summarizes the overall performance of the model, as represented by the ROC curve. Ranges from 0 to 1, with higher values indicating better performance.\n",
        "\n",
        "* **Why it's important:** Provides a way to compare different models.\n",
        "\n",
        "**Which metric to choose?**\n",
        "\n",
        "The best metric depends on your specific problem and priorities.\n",
        "\n",
        "* **Balanced classes:** Accuracy is often a good starting point.\n",
        "\n",
        "* **Imbalanced classes:** Precision, recall, or F1-score are more informative.\n",
        "\n",
        "* **Minimizing false positives:** Precision is crucial.\n",
        "\n",
        "* **Minimizing false negatives:** Recall is crucial.\n",
        "\n",
        "* **Trade-off between precision and recall:** F1-score is a good choice.\n",
        "Overall performance across thresholds: AUC is useful.\n",
        "\n",
        "It's often helpful to consider multiple metrics to get a comprehensive understanding of your model's performance.\n",
        "     "
      ],
      "metadata": {
        "id": "6nuhO1ZDgr64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###12. How does class imbalance affect Logistic Regression ?\n",
        "\n",
        "Ans-12. Class imbalance, where one class has significantly more examples than another, can significantly impact logistic regression (and other classification algorithms). Here's how:\n",
        "\n",
        "1. **Biased Predictions:**\n",
        "\n",
        "* **The Problem:** A model trained on imbalanced data tends to be biased towards the majority class. It might predict the majority class even when the input actually belongs to the minority class, simply because it has seen so many more examples of the majority class during training.\n",
        "\n",
        "* **Why it happens:** The model tries to minimize the overall error, and it's easier to achieve low error by correctly classifying the majority class most of the time, even at the expense of misclassifying the minority class.\n",
        "\n",
        "2. **Poor Performance on the Minority Class:**\n",
        "\n",
        "* **The Problem:** The minority class is often the class of greater interest (e.g., disease diagnosis, fraud detection). Due to the bias mentioned above, the model might have poor recall (ability to correctly identify positive cases) on the minority class.\n",
        "\n",
        "* **Why it matters:**  In many real-world applications, accurately identifying the minority class is crucial, even if it means misclassifying some examples from the majority class.\n",
        "\n",
        "3. **Misleading Accuracy:**\n",
        "\n",
        "* **The Problem:** Accuracy can be a misleading metric when dealing with imbalanced data. A model that always predicts the majority class can achieve high accuracy, even if it's completely useless for identifying the minority class.\n",
        "\n",
        "* **Why it's a problem:** Relying solely on accuracy can give a false sense of confidence in the model's performance.\n",
        "\n",
        "4. **Problems with Optimization:**\n",
        "\n",
        "* **The Problem:** Some optimization algorithms used to train logistic regression models can struggle with imbalanced data. They might converge to a solution that favors the majority class, even if it's not the optimal solution for the problem.\n",
        "\n",
        "**How to Address Class Imbalance:**\n",
        "\n",
        "Several techniques can be used to mitigate the effects of class imbalance:\n",
        "\n",
        "* Resampling:\n",
        "\n",
        "   * **Oversampling:** Creating copies of the minority class examples. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can generate synthetic examples.\n",
        "\n",
        "   * **Undersampling:** Removing examples from the majority class.\n",
        "\n",
        "* **Cost-Sensitive Learning:** Assigning higher misclassification costs to the minority class during training. This forces the model to pay more attention to correctly classifying the minority class.\n",
        "\n",
        "* **Class Weights:** Similar to cost-sensitive learning, assigning weights to the classes during training, with higher weights for the minority class.\n",
        "\n",
        "* **Ensemble Methods:** Techniques like bagging and boosting can be adapted to handle class imbalance. For example, Balanced Random Forests oversample the minority class in each tree.\n",
        "\n",
        "* **Evaluation Metrics:** Using appropriate evaluation metrics like precision, recall, F1-score, AUC, and the confusion matrix is crucial. These metrics provide a more informative picture of the model's performance on both classes.\n",
        "\n",
        "**In short:** Class imbalance can significantly hinder the performance of logistic regression, particularly on the minority class. It's essential to be aware of this issue and employ appropriate techniques to address it, ensuring that the model is effectively learning from all classes."
      ],
      "metadata": {
        "id": "Y0PNX4XlkFnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13. What is Hyperparameter Tuning in Logistic Regression ?\n",
        "\n",
        "Ans-13. Hyperparameter tuning in logistic regression is a crucial step to improve the performance of the model by finding the best values for hyperparameters that control the learning process. Unlike regular parameters (weights and biases) that the model learns from the training data, hyperparameters are set before the training process begins.\n",
        "\n",
        "In logistic regression, the main hyperparameter is the regularization parameter, which controls the complexity of the model to prevent overfitting. There are two common types of regularization:\n",
        "\n",
        "1. **L1 regularization (Lasso):** This adds a penalty equal to the absolute value of the magnitude of coefficients. It tends to produce sparse models with few coefficients, effectively selecting a simpler model.\n",
        "\n",
        "2. **L2 regularization (Ridge):** This adds a penalty equal to the square of the magnitude of coefficients. It encourages the coefficients to be small, but not exactly zero, which helps in controlling the complexity of the model.\n",
        "\n",
        "To find the best value for these hyperparameters, one can use techniques like:\n",
        "\n",
        "* **Grid Search:** It involves searching over a predefined set of hyperparameters and evaluating the model performance for each combination using cross-validation. This method is exhaustive but can be computationally expensive.\n",
        "\n",
        "* **Random Search:** Instead of searching over a predefined grid, random search samples hyperparameters from a specified distribution. It often leads to better results with less computational effort.\n",
        "\n",
        "* **Bayesian Optimization:** This method uses a probabilistic model to choose the next set of hyperparameters to evaluate, aiming to find the best values more efficiently than grid or random search.\n",
        "\n",
        "By carefully tuning these hyperparameters, the logistic regression model can achieve better performance on unseen data."
      ],
      "metadata": {
        "id": "i5jkMpG2BdIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14.What are different solvers in Logistic Regression? Which one should be used ?\n",
        "\n",
        "Ans-14. In logistic regression, solvers are algorithms used to optimize the objective function. Different solvers have their own strengths and weaknesses, and choosing the right one depends on the specific characteristics of the dataset and the problem at hand. Here are some of the common solvers used in logistic regression:\n",
        "\n",
        "1.**liblinear:** A good choice for small datasets. It uses a coordinate descent algorithm and is suitable for both L1 and L2 regularization.\n",
        "\n",
        "2. **newton-cg:** A second-order algorithm that uses the Newton conjugate gradient method. It's suitable for large datasets and is effective for solving multi-class problems but may be slower than other solvers.\n",
        "\n",
        "3. **lbfgs:** Stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm. It's a quasi-Newton method that approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm using limited memory. It's efficient for large datasets and multi-class problems and works well with L2 regularization.\n",
        "\n",
        "4. **sag:** Stands for Stochastic Average Gradient. It's a first-order optimization algorithm that scales well with large datasets. It supports only L2 regularization and is faster than some second-order methods like newton-cg.\n",
        "\n",
        "5. **saga:** An improved version of the SAG solver, supporting both L1 and L2 regularization. It's suitable for large datasets and is particularly useful for sparse data. It also supports the elastic net regularization.\n",
        "\n",
        "####Which solver to use? The choice of solver depends on several factors:\n",
        "\n",
        "* **Dataset Size:** For small datasets, liblinear is a good choice. For large datasets, lbfgs, sag, or saga might be more efficient.\n",
        "\n",
        "* **Regularization Type:** If you need L1 regularization, you should use liblinear or saga. For L2 regularization, you can use any solver, but lbfgs, newton-cg, sag, and saga are generally preferred.\n",
        "\n",
        "* **Multi-class Problems:** If you have a multi-class problem, lbfgs, newton-cg, and saga are good options since they handle multi-class classification well.\n",
        "\n",
        "Ultimately, it's often beneficial to try a few different solvers and compare their performance on your specific problem."
      ],
      "metadata": {
        "id": "mt-LGI-RCTRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15.How is Logistic Regression extended for multiclass classification ?\n",
        "\n",
        "Ans-15. Logistic Regression, in its basic form, is designed for binary classification problems (predicting one of two outcomes). To extend it for multi-class classification (predicting one of multiple outcomes), we use techniques like:\n",
        "\n",
        "1. **One-vs-All (OvA) or One-vs-Rest:**\n",
        "\n",
        "* **Concept:** Train a separate logistic regression classifier for each class. Each classifier predicts the probability of an instance belonging to that class versus all other classes.\n",
        "\n",
        "* **Prediction:** Choose the class with the highest probability among all classifiers.\n",
        "\n",
        "* **Advantages:** Simple to implement.\n",
        "\n",
        "* **Disadvantages:** Can lead to class imbalance issues if one class has significantly more instances than others.\n",
        "\n",
        "2. **Multinomial Logistic Regression (Softmax Regression):**\n",
        "\n",
        "* **Concept:** Generalizes logistic regression by directly modeling the probabilities of all classes simultaneously. It uses the softmax function, which is a generalization of the sigmoid function, to produce a probability distribution over all classes.\n",
        "\n",
        "* **Prediction:** Choose the class with the highest predicted probability.\n",
        "Advantages: More statistically sound than OvA, as it models all classes at once.\n",
        "Disadvantages: Computationally more complex than OvA.\n",
        "\n",
        "3. **Hierarchical Logistic Regression:**\n",
        "\n",
        "* **Concept:** Organizes classes into a hierarchy (tree-like structure). Classifiers are trained at each level of the hierarchy to make decisions about which branch to take, eventually leading to a specific class.\n",
        "\n",
        "* **Advantages:** Can be efficient if the class hierarchy reflects the data well.\n",
        "\n",
        "* **Disadvantages:** Requires careful design of the class hierarchy.\n",
        "\n",
        "####Key Differences and Considerations:\n",
        "\n",
        "* **Softmax Regression** is generally preferred when the classes are mutually exclusive (an instance can belong to only one class).\n",
        "\n",
        "* **One-vs-All** can be used when classes are not mutually exclusive (an instance can belong to multiple classes).\n",
        "\n",
        "* The choice of method can depend on the specific problem, the nature of the data, and the computational resources available.\n",
        "\n",
        "**Note:** In practice, libraries like scikit-learn in Python provide built-in implementations of these multi-class logistic regression techniques, making it easy to use them without having to implement them from scratch."
      ],
      "metadata": {
        "id": "Bnqr7wI_DlXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16. What are the advantages and disadvantages of Logistic Regression ?\n",
        "\n",
        "Ans-16. Logistic Regression is a popular and widely used algorithm for binary classification problems. It's appreciated for its simplicity and interpretability, but it also has limitations. Here's a breakdown of its advantages and disadvantages:\n",
        "\n",
        "####**Advantages of Logistic Regression:**\n",
        "\n",
        "* **Easy to Implement and Interpret:** Logistic regression is relatively simple to understand and implement. The coefficients of the model can be interpreted as the importance of each feature in predicting the outcome. This interpretability is a significant advantage, especially in fields like healthcare and finance where understanding the model's reasoning is crucial.\n",
        "\n",
        "* **Efficient to Train:** Logistic regression models train quickly, even on large datasets. This makes it a good choice for situations where you need a fast and efficient solution.\n",
        "\n",
        "* **Handles Various Data Types:** It can handle both continuous and categorical features, making it versatile for different types of data.\n",
        "\n",
        "* **Provides Probability Estimates:** Logistic regression outputs probabilities of the instance belonging to a particular class. This can be useful for decision-making, as you can set thresholds based on the desired level of confidence.\n",
        "\n",
        "* **Well-Calibrated Probabilities:** The probability estimates produced by logistic regression are often well-calibrated, meaning they accurately reflect the true likelihood of an event occurring.\n",
        "\n",
        "* **Effective Baseline:** It serves as a good baseline model to compare against more complex algorithms.\n",
        "\n",
        "####**Disadvantages of Logistic Regression:**\n",
        "\n",
        "* **Assumes Linearity:** Logistic regression assumes a linear relationship between the features and the log-odds of the outcome. This assumption may not hold in real-world scenarios where the relationships are more complex.\n",
        "\n",
        "* **Sensitive to Multicollinearity:** Multicollinearity (high correlation between independent variables) can negatively impact the model's performance and make the coefficients unstable.\n",
        "\n",
        "* **Can Overfit:** Like any model, logistic regression can overfit the training data, especially with high-dimensional datasets. Regularization techniques can help mitigate this issue.\n",
        "\n",
        "* **Limited to Linearly Separable Data:** It performs best when the classes are linearly separable, meaning they can be separated by a straight line or hyperplane. It may struggle with more complex, non-linearly separable data.\n",
        "Not Suitable for Complex Relationships: Logistic regression may not be the best choice for problems with complex relationships between features and outcomes. In such cases, more powerful algorithms like decision trees or neural networks might be more appropriate.\n",
        "\n",
        "**In summary:**\n",
        "Logistic regression is a valuable tool for binary classification tasks, especially when interpretability and efficiency are important. However, it's essential to be aware of its limitations and consider whether the assumptions of linearity and linear separability are met by the data."
      ],
      "metadata": {
        "id": "fwm8g_UJFRBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17. What are some use cases of Logistic Regression ?\n",
        "\n",
        "Ans-17. Logistic Regression is quite versatile and is widely used in various fields.\n",
        "\n",
        "**Here are some common use cases:**\n",
        "\n",
        "1. **Medical Diagnosis:**\n",
        "\n",
        "* Predicting the presence of diseases based on patient features such as symptoms, medical history, and test results. For example, it can be used to predict the likelihood of heart disease or diabetes.\n",
        "\n",
        "2. **Customer Churn Prediction:**\n",
        "\n",
        "* Identifying customers who are likely to stop using a product or service based on their usage patterns and other relevant features.\n",
        "\n",
        "3. **Credit Scoring:**\n",
        "\n",
        "* Assessing the probability of a borrower defaulting on a loan based on their credit history, income, and other financial factors.\n",
        "\n",
        "4. **Spam Detection:**\n",
        "\n",
        "* Classifying emails as spam or not spam based on features like the content of the email, sender information, and other metadata.\n",
        "\n",
        "5. **Marketing and Sales:**\n",
        "\n",
        "* Predicting the probability of a customer making a purchase based on their browsing history, demographic information, and other factors.\n",
        "\n",
        "* Identifying the likelihood of a customer responding to a marketing campaign.\n",
        "\n",
        "6. **Employee Attrition:**\n",
        "\n",
        "* Predicting which employees are likely to leave the company based on factors like job satisfaction, salary, work environment, and performance metrics.\n",
        "\n",
        "7. **Fraud Detection:**\n",
        "\n",
        "* Identifying fraudulent transactions in banking or e-commerce by analyzing patterns in transaction data.\n",
        "\n",
        "8. **Social Science Research:**\n",
        "\n",
        "* Analyzing survey data to understand the likelihood of certain behaviors or opinions based on demographic and psychographic variables.\n",
        "\n",
        "**These are just a few examples.** Logistic Regression is a fundamental tool in the data scientist's toolkit, thanks to its simplicity, interpretability, and effectiveness for binary classification problems."
      ],
      "metadata": {
        "id": "onp953tlGZul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###18.What is the difference between Softmax Regression and Logistic Regression ?\n",
        "\n",
        "Ans-18. Here's a breakdown of the key differences between Softmax Regression and Logistic Regression:\n",
        "\n",
        "1. **Number of Classes:**\n",
        "\n",
        "* **Logistic Regression:** Designed for binary classification problems, where the outcome variable has only two possible classes (e.g., yes/no, 0/1, spam/not spam).\n",
        "\n",
        "* **Softmax Regression:** A generalization of logistic regression for multi-class classification problems, where the outcome variable can have more than two classes (e.g., classifying images of different animals, predicting which of several products a customer will buy).\n",
        "\n",
        "2. **Output Function:**\n",
        "\n",
        "* **Logistic Regression:** Uses the sigmoid function to produce a probability between 0 and 1, representing the probability of an instance belonging to one of the two classes.\n",
        "\n",
        "* **Softmax Regression:** Uses the softmax function to produce a probability distribution over all the classes. The softmax function outputs a vector of probabilities, where each element represents the probability of an instance belonging to a specific class. These probabilities sum up to 1.\n",
        "\n",
        "3. **Decision Boundary:**\n",
        "\n",
        "* **Logistic Regression:** Creates a linear decision boundary to separate the two classes.\n",
        "\n",
        "* **Softmax Regression:** Creates multiple linear decision boundaries, one for each class, to separate the classes.\n",
        "\n",
        "4. **Relationship:**\n",
        "\n",
        "* Softmax Regression can be considered a generalization of Logistic Regression. In fact, when you have only two classes, Softmax Regression is essentially equivalent to Logistic Regression.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "* If you're dealing with a binary classification problem (two classes), use Logistic Regression.\n",
        "\n",
        "* If you're dealing with a multi-class classification problem (more than two classes), use Softmax Regression.\n",
        "\n",
        "Think of it this way: Logistic Regression is like flipping a coin (two outcomes), while Softmax Regression is like rolling a die (multiple outcomes)."
      ],
      "metadata": {
        "id": "KQF4z7ncHmIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification ?\n",
        "\n",
        "Ans-19. Choosing between One-vs-Rest (OvR) and Softmax for multiclass classification depends on several factors, such as the nature of your data, the complexity of your problem, and your computational constraints. Here's a breakdown to help you decide:\n",
        "\n",
        "###One-vs-Rest (OvR)\n",
        "\n",
        "* **What It Is:** OvR involves training a separate binary classifier for each class. Each classifier predicts whether a sample belongs to its class or not.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* **Simplicity:** OvR is conceptually simpler and often easier to implement.\n",
        "\n",
        "* **Interpretability:** You get separate classifiers for each class, making it easier to interpret the individual models' behavior.\n",
        "\n",
        "* **Flexibility:** OvR can handle imbalanced datasets better because each classifier focuses on distinguishing one class from the rest.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* **Scalability:** Training separate classifiers for each class can be computationally expensive for large datasets with many classes.\n",
        "\n",
        "* **Conflict:** Different classifiers might make conflicting predictions.\n",
        "\n",
        "###Softmax\n",
        "\n",
        "* **What It Is:** Softmax is used in neural networks (e.g., logistic regression and deep learning). It outputs probabilities for each class, and the class with the highest probability is chosen.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* **Efficiency:** Softmax trains a single model, which is computationally efficient, especially for neural networks.\n",
        "\n",
        "* **Probabilistic Interpretation:** Provides probabilities for each class, which can be useful for uncertainty estimation and decision-making.\n",
        "\n",
        "* **Consistency:** The predictions are consistent and do not conflict, as the model is trained jointly on all classes.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* **Complexity:** Implementing Softmax can be more complex, especially for non-neural network models.\n",
        "\n",
        "* **Sensitivity to Imbalance:** Softmax may struggle with class imbalance as it treats all classes equally during training.\n",
        "\n",
        "###When to Use Each\n",
        "\n",
        "### **One-vs-Rest (OvR):**\n",
        "\n",
        "* When you have a small to moderate number of classes.\n",
        "\n",
        "* When interpretability of individual class models is important.\n",
        "\n",
        "* When dealing with imbalanced datasets.\n",
        "\n",
        "### **Softmax:**\n",
        "\n",
        "* When you have a large number of classes.\n",
        "\n",
        "* When computational efficiency is a priority.\n",
        "\n",
        "* When you need probabilistic outputs for decision-making.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "* **OvR:** Simpler to implement, suitable for non-mutually exclusive classes or potentially large numbers of classes.\n",
        "\n",
        "* **Softmax:** More statistically sound, provides better probability estimates, generally preferred for mutually exclusive classes.\n",
        "\n",
        "Ultimately, the best choice depends on the specific problem, the nature of the data, and the trade-offs you're willing to make between accuracy, computational cost, and interpretability."
      ],
      "metadata": {
        "id": "2H1AwrVXI78E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###20.How do we interpret coefficients in Logistic Regression ?\n",
        "\n",
        "Ans-20. Interpreting coefficients in logistic regression requires a bit more nuance than in linear regression because the outcome variable is not continuous. Here's a breakdown of how to interpret them, along with some important considerations:\n",
        "\n",
        "1. **The Basics:**\n",
        "\n",
        "* **Coefficients and Log-Odds:** Logistic regression models the relationship between the independent variables and the log-odds of the outcome. The log-odds are the natural logarithm of the odds, where odds = probability of the event occurring / probability of the event not occurring.\n",
        "\n",
        "* **Direction:**\n",
        "\n",
        "* A positive coefficient means that an increase in the predictor variable is associated with an increase in the log-odds of the outcome (making the outcome more likely).\n",
        "\n",
        "* A negative coefficient means that an increase in the predictor variable is associated with a decrease in the log-odds of the outcome (making the outcome less likely).\n",
        "\n",
        "2. **Converting to Odds Ratios:**\n",
        "\n",
        "* Exponentiating the Coefficient: To make the interpretation more intuitive, we often exponentiate the coefficient (e^coefficient). This gives us the odds ratio.\n",
        "\n",
        "* **Interpreting the Odds Ratio:**\n",
        "\n",
        "* An odds ratio greater than 1 means that a one-unit increase in the predictor variable multiplies the odds of the outcome by that value. For example, an odds ratio of 2 means that a one-unit increase in the predictor doubles the odds of the outcome.\n",
        "\n",
        "* An odds ratio less than 1 means that a one-unit increase in the predictor variable divides the odds of the outcome by that value. For example, an odds ratio of 0.5 means that a one-unit increase in the predictor halves the odds of the outcome.\n",
        "\n",
        "* An odds ratio of 1 means that the predictor variable has no effect on the odds of the outcome.\n",
        "\n",
        "3. **Example:**\n",
        "\n",
        "Let's say we're modeling the probability of a customer clicking on an ad based on their age. The logistic regression coefficient for age is 0.05.\n",
        "\n",
        "* **Log-Odds:** For every one-year increase in age, the log-odds of clicking on the ad increase by 0.05.\n",
        "\n",
        "* **Odds Ratio:** e^0.05 ≈ 1.05. For every one-year increase in age, the odds of clicking on the ad are multiplied by 1.05 (a 5% increase).\n",
        "\n",
        "4. **Important Considerations:**\n",
        "\n",
        "* **Scale of the Predictor:** The interpretation of the coefficient depends on the scale of the predictor variable. A one-unit change in age might be different from a one-unit change in income.\n",
        "\n",
        "* **Other Variables:** The interpretation of a coefficient assumes that all other variables in the model are held constant.\n",
        "\n",
        "* **Statistical Significance:** It's crucial to consider the statistical significance of the coefficient (p-value). A statistically significant coefficient indicates that the relationship between the predictor and the outcome is unlikely to be due to chance.\n",
        "\n",
        "* **Causality:** Logistic regression shows association, not causation. A significant coefficient doesn't necessarily mean that the predictor causes the outcome.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Interpreting logistic regression coefficients involves understanding their relationship with log-odds and converting them to odds ratios for a more intuitive interpretation. Always consider the scale of the predictor, other variables in the model, statistical significance, and the limitations of association vs. causation.\n"
      ],
      "metadata": {
        "id": "AYFddLJJMYRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "MfE4pAQkJ-FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy."
      ],
      "metadata": {
        "id": "119Ns4AZKEfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (for this example, we'll use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCJBf_pdKPYs",
        "outputId": "515df218-9d9a-4d3a-dde1-55ad7712c47a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy."
      ],
      "metadata": {
        "id": "oqx3XTs_KYu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (for this example, we'll use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with L1 regularization: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imZRZ283KgJm",
        "outputId": "6607ab06-f530-437e-9356-aab17622f8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 regularization: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "Nw6xIXR9K0ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (for this example, we'll use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with L2 regularization: {accuracy:.2f}')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11ttHJldK8Xp",
        "outputId": "fec844fc-79e9-4c23-a9d7-730c8c2157c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 regularization: 0.98\n",
            "Model Coefficients:\n",
            "[[ 0.36479402  1.35499766 -2.09628559 -0.92154751]\n",
            " [ 0.4808915  -1.58463288  0.3937527  -1.09224057]\n",
            " [-1.5286415  -1.43244729  2.3048277   2.08584535]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "NUQqWZyeLI_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (for this example, we'll use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with Elastic Net regularization: {accuracy:.2f}')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnGLDLFLLRvV",
        "outputId": "56f631df-efa4-439f-fc3d-ddf06e1c1d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net regularization: 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.38694657  1.71275573 -2.35225066 -0.65591941]\n",
            " [ 0.04404239  0.          0.         -0.54156348]\n",
            " [-1.2244199  -1.29287829  2.46962086  1.99017254]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'."
      ],
      "metadata": {
        "id": "EIPf7R4fLabO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (for this example, we'll use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply Logistic Regression for multiclass classification with one-vs-rest\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with one-vs-rest: {accuracy:.2f}')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq2CMQVILmWT",
        "outputId": "1f6e1964-67e6-49b8-8646-fedd41ded33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with one-vs-rest: 0.98\n",
            "Model Coefficients:\n",
            "[[ 0.36479402  1.35499766 -2.09628559 -0.92154751]\n",
            " [ 0.4808915  -1.58463288  0.3937527  -1.09224057]\n",
            " [-1.5286415  -1.43244729  2.3048277   2.08584535]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "K932ujaJL7KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (for this example, we'll use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both l1 and l2 penalties\n",
        "}\n",
        "\n",
        "# Apply Logistic Regression with GridSearchCV to tune hyperparameters\n",
        "model = LogisticRegression(max_iter=200)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Best Cross-Validated Accuracy: {best_score:.2f}')\n",
        "\n",
        "# Evaluate the model with the best parameters on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy on Test Set: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF57TtubMwJ1",
        "outputId": "d6c61eac-cec8-4b52-b74e-f9b3a3d24a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best Cross-Validated Accuracy: 0.95\n",
            "Model Accuracy on Test Set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy."
      ],
      "metadata": {
        "id": "24OwtcoXM8MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (for this example, we'll use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Initialize Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Initialize variables to store results\n",
        "accuracies = []\n",
        "\n",
        "# Loop over each fold\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Apply Logistic Regression\n",
        "    model = LogisticRegression(max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy and store it\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate and print the average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(f'Average Accuracy with Stratified K-Fold Cross-Validation: {average_accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzTe812ONIWg",
        "outputId": "c86bceb4-49ce-4c2c-81df-526105d5e4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy with Stratified K-Fold Cross-Validation: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy."
      ],
      "metadata": {
        "id": "iSEWCGY0NOyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "file_path = 'your_file.csv'\n",
        "\n",
        "try:\n",
        "    # Load the dataset from the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Assuming the last column is the target variable\n",
        "    X = df.iloc[:, :-1]  # Features\n",
        "    y = df.iloc[:, -1]    # Target variable\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create and train the Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model's accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy of the Logistic Regression model: {accuracy}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{file_path}' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4i22Go4Nv1x",
        "outputId": "5011543c-7c16-4916-91aa-bd42dac68179"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File 'your_file.csv' not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "9cdr0nN8NWol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 3, 7),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'] # 'lbfgs', 'newton-cg' are incompatible with l1 penalty\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42, max_iter=10000)\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(\n",
        "    logistic_regression,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,  # Number of parameter combinations to try\n",
        "    cv=5,        # Number of cross-validation folds\n",
        "    random_state=42,\n",
        "    n_jobs=-1,  # Use all available CPU cores\n",
        "    verbose=1,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Cross-Validation Accuracy:\", best_score)\n",
        "\n",
        "# Evaluate the model on the test set using the best parameters\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stTqt8alOywo",
        "outputId": "9425cd05-591e-4313-9bd5-bbeee133c90c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 28 is smaller than n_iter=50. Running 28 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.1}\n",
            "Best Cross-Validation Accuracy: 0.875\n",
            "Test Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "x9HtHhm5PSA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "\n",
        "class OneVsOneLogisticRegression:\n",
        "    def __init__(self, random_state=None):\n",
        "        self.classifiers = {}\n",
        "        self.classes = None\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        for class1, class2 in combinations(self.classes, 2):\n",
        "            binary_mask = np.logical_or(y == class1, y == class2)\n",
        "            X_binary = X[binary_mask]\n",
        "            y_binary = y[binary_mask]\n",
        "\n",
        "            # Convert labels to binary (0 and 1)\n",
        "            y_binary = np.where(y_binary == class1, 0, 1)\n",
        "\n",
        "            classifier = LogisticRegression(random_state=self.random_state)\n",
        "            classifier.fit(X_binary, y_binary)\n",
        "            self.classifiers[(class1, class2)] = classifier\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.classes is None:\n",
        "            raise ValueError(\"Model not fitted yet. Call fit() first.\")\n",
        "\n",
        "        num_samples = X.shape[0]\n",
        "        votes = np.zeros((num_samples, len(self.classes)))\n",
        "\n",
        "        for (class1, class2), classifier in self.classifiers.items():\n",
        "            predictions = classifier.predict(X)\n",
        "            for i, prediction in enumerate(predictions):\n",
        "                if prediction == 0:\n",
        "                    votes[i, np.where(self.classes == class1)[0][0]] += 1\n",
        "                else:\n",
        "                    votes[i, np.where(self.classes == class2)[0][0]] += 1\n",
        "\n",
        "        return self.classes[np.argmax(votes, axis=1)]\n",
        "\n",
        "# Example usage:\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_classes=4, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "ovo_model = OneVsOneLogisticRegression(random_state=42)\n",
        "ovo_model.fit(X_train, y_train)\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"One-vs-One Logistic Regression Accuracy:\", accuracy)\n",
        "\n",
        "# Compare with sklearn's OvR (One vs Rest) Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "ovr_model = OneVsRestClassifier(LogisticRegression(random_state=42))\n",
        "ovr_model.fit(X_train, y_train)\n",
        "y_pred_ovr = ovr_model.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(\"One-vs-Rest Logistic Regression Accuracy (sklearn):\", accuracy_ovr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gsw45z4PdNc",
        "outputId": "db5797d8-1175-4a2d-9104-da02412b5185"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 0.635\n",
            "One-vs-Rest Logistic Regression Accuracy (sklearn): 0.615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification."
      ],
      "metadata": {
        "id": "PSVGLzmePpez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Visualize the confusion matrix using seaborn and matplotlib\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "ga10qg26QLUj",
        "outputId": "462cfbe8-c125-46f4-e2a3-1d0c4df76032"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8800\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score."
      ],
      "metadata": {
        "id": "4envQBbgQOq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42, max_iter=1000)  # Increase max_iter if needed\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t03J9En8QW0U",
        "outputId": "65f395be-adf2-4815-9fa1-8464df46e8db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9149\n",
            "Recall: 0.8037\n",
            "F1-Score: 0.8557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance."
      ],
      "metadata": {
        "id": "eVjs-0iUQgwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42, weights=[0.9, 0.1])  # 90% class 0, 10% class 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model without class weights\n",
        "logistic_regression_no_weights = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = logistic_regression_no_weights.predict(X_test)\n",
        "\n",
        "print(\"Logistic Regression without class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "# Train a Logistic Regression model with class weights\n",
        "logistic_regression_weights = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
        "logistic_regression_weights.fit(X_train, y_train)\n",
        "y_pred_weights = logistic_regression_weights.predict(X_test)\n",
        "\n",
        "print(\"\\nLogistic Regression with class weights:\")\n",
        "print(classification_report(y_test, y_pred_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2Dckb9MQ4Of",
        "outputId": "a503cbbd-48bf-41ca-8770-d37ebce1bae1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression without class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.97       182\n",
            "           1       0.75      0.50      0.60        18\n",
            "\n",
            "    accuracy                           0.94       200\n",
            "   macro avg       0.85      0.74      0.78       200\n",
            "weighted avg       0.93      0.94      0.93       200\n",
            "\n",
            "\n",
            "Logistic Regression with class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.82      0.89       182\n",
            "           1       0.31      0.83      0.45        18\n",
            "\n",
            "    accuracy                           0.82       200\n",
            "   macro avg       0.65      0.83      0.67       200\n",
            "weighted avg       0.92      0.82      0.85       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance."
      ],
      "metadata": {
        "id": "umwtpX5QRHO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "data = pd.read_csv('path_to_titanic_dataset.csv')\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data['Age'] = imputer.fit_transform(data[['Age']])\n",
        "\n",
        "# Drop rows with missing values in the 'Embarked' column\n",
        "data.dropna(subset=['Embarked'], inplace=True)\n",
        "\n",
        "# Convert categorical features to numeric\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Select features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "X = data[features]\n",
        "y = data['Survived']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laKx5rdZdIR4",
        "outputId": "566ff91c-fb2f-45e5-b249-980122268ecd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "DIgb0kGaSQTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "logistic_regression_no_scaling = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = logistic_regression_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "# Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "logistic_regression_scaled = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = logistic_regression_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "#Compare the results.\n",
        "if accuracy_scaled > accuracy_no_scaling:\n",
        "    print(\"Scaling improved accuracy.\")\n",
        "elif accuracy_scaled < accuracy_no_scaling:\n",
        "    print(\"Scaling decreased accuracy.\")\n",
        "else:\n",
        "    print(\"Scaling didn't change accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQmravAcSXnu",
        "outputId": "18c963a0-b694-460e-eb61-55a9082759dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.8550\n",
            "Accuracy with scaling: 0.8550\n",
            "Scaling didn't change accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "Q-mI1HjrShlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_prob = logistic_regression.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWN87l8PSnh9",
        "outputId": "ffec4f45-c75c-415a-be18-252de6d518c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy."
      ],
      "metadata": {
        "id": "vjUYxKurSt3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Logistic Regression model with custom C (inverse of regularization strength)\n",
        "logistic_regression = LogisticRegression(C=0.5, random_state=42, max_iter=1000)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.4f}\")\n",
        "\n",
        "#Demonstration of how C works.\n",
        "logistic_regression_default = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression_default.fit(X_train, y_train)\n",
        "y_pred_default = logistic_regression_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "print(f\"Accuracy with default C=1.0: {accuracy_default:.4f}\")\n",
        "\n",
        "logistic_regression_very_strong = LogisticRegression(C=1000.0, random_state=42, max_iter=1000)\n",
        "logistic_regression_very_strong.fit(X_train, y_train)\n",
        "y_pred_very_strong = logistic_regression_very_strong.predict(X_test)\n",
        "accuracy_very_strong = accuracy_score(y_test, y_pred_very_strong)\n",
        "print(f\"Accuracy with C=1000.0: {accuracy_very_strong:.4f}\")\n",
        "\n",
        "logistic_regression_very_weak = LogisticRegression(C=0.001, random_state=42, max_iter=1000)\n",
        "logistic_regression_very_weak.fit(X_train, y_train)\n",
        "y_pred_very_weak = logistic_regression_very_weak.predict(X_test)\n",
        "accuracy_very_weak = accuracy_score(y_test, y_pred_very_weak)\n",
        "print(f\"Accuracy with C=0.001: {accuracy_very_weak:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yUR8zwDS016",
        "outputId": "389f41c3-561f-43bb-ac7f-4702b1399ba8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.8550\n",
            "Accuracy with default C=1.0: 0.8550\n",
            "Accuracy with C=1000.0: 0.8550\n",
            "Accuracy with C=0.001: 0.8300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###18. Write a Python program to train Logistic Regression and identify important features based on model coefficients."
      ],
      "metadata": {
        "id": "02ZnlsQYTCNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic dataset with feature names\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42, n_informative=5)\n",
        "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "X_df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Get feature coefficients\n",
        "coefficients = logistic_regression.coef_[0]\n",
        "\n",
        "# Create a DataFrame to display feature importance\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "# Sort features by absolute coefficient value (magnitude)\n",
        "feature_importance['Absolute Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Print the feature importance\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']])\n",
        "\n",
        "#Optional: Print the most influential features.\n",
        "print(\"\\nMost influential features:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']].head(5)) #display top 5."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkEZFPcYTLO4",
        "outputId": "01884ec9-e816-40a2-ae8b-0fd95cf0a76d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance:\n",
            "     Feature  Coefficient\n",
            "3  feature_3     0.818078\n",
            "0  feature_0     0.697105\n",
            "5  feature_5    -0.679813\n",
            "4  feature_4    -0.632422\n",
            "1  feature_1    -0.253707\n",
            "7  feature_7    -0.103886\n",
            "6  feature_6    -0.080110\n",
            "8  feature_8     0.047178\n",
            "2  feature_2     0.046884\n",
            "9  feature_9    -0.025688\n",
            "\n",
            "Most influential features:\n",
            "     Feature  Coefficient\n",
            "3  feature_3     0.818078\n",
            "0  feature_0     0.697105\n",
            "5  feature_5    -0.679813\n",
            "4  feature_4    -0.632422\n",
            "1  feature_1    -0.253707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa Score."
      ],
      "metadata": {
        "id": "SJGUBKnKVavm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print the Cohen's Kappa score\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ0vWvwrVlOc",
        "outputId": "c3044bee-3ab8-4af6-df42-fa67959858b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.7112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio."
      ],
      "metadata": {
        "id": "HAShXKUsVrkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_prob = logistic_regression.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Calculate the area under the Precision-Recall curve (AUC-PR)\n",
        "auc_pr = auc(recall, precision)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AUC-PR = {auc_pr:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "I5WHSNKpV0X6",
        "outputId": "b7f335f1-0482-41fa-ddc3-37f926506fc1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy."
      ],
      "metadata": {
        "id": "gkS781uZV8nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Solvers to try\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        # Create and train a Logistic Regression model with the current solver\n",
        "        logistic_regression = LogisticRegression(solver=solver, random_state=42, max_iter=1000)\n",
        "        logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "        # Calculate and print the accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy with solver '{solver}': {accuracy:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Solver '{solver}' encountered an error: {e}\")\n",
        "\n",
        "#Demonstration of how penalties affect solvers.\n",
        "print(\"\\nDemonstration of penalty affect on solvers:\")\n",
        "\n",
        "try:\n",
        "    logistic_regression_l1_liblinear = LogisticRegression(solver='liblinear', penalty='l1', random_state=42, max_iter=1000)\n",
        "    logistic_regression_l1_liblinear.fit(X_train, y_train)\n",
        "    y_pred_l1_liblinear = logistic_regression_l1_liblinear.predict(X_test)\n",
        "    accuracy_l1_liblinear = accuracy_score(y_test, y_pred_l1_liblinear)\n",
        "    print(f\"Accuracy with solver 'liblinear' and penalty 'l1': {accuracy_l1_liblinear:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Solver 'liblinear' with penalty 'l1' encountered an error: {e}\")\n",
        "\n",
        "try:\n",
        "    logistic_regression_l1_saga = LogisticRegression(solver='saga', penalty='l1', random_state=42, max_iter=1000)\n",
        "    logistic_regression_l1_saga.fit(X_train, y_train)\n",
        "    y_pred_l1_saga = logistic_regression_l1_saga.predict(X_test)\n",
        "    accuracy_l1_saga = accuracy_score(y_test, y_pred_l1_saga)\n",
        "    print(f\"Accuracy with solver 'saga' and penalty 'l1': {accuracy_l1_saga:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Solver 'saga' with penalty 'l1' encountered an error: {e}\")\n",
        "\n",
        "try:\n",
        "    logistic_regression_l1_lbfgs = LogisticRegression(solver='lbfgs', penalty='l1', random_state=42, max_iter=1000)\n",
        "    logistic_regression_l1_lbfgs.fit(X_train, y_train)\n",
        "    y_pred_l1_lbfgs = logistic_regression_l1_lbfgs.predict(X_test)\n",
        "    accuracy_l1_lbfgs = accuracy_score(y_test, y_pred_l1_lbfgs)\n",
        "    print(f\"Accuracy with solver 'lbfgs' and penalty 'l1': {accuracy_l1_lbfgs:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Solver 'lbfgs' with penalty 'l1' encountered an error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpPyatSKWDMp",
        "outputId": "dfe23098-ee29-4694-e33d-dd90d46ab5c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with solver 'liblinear': 0.8550\n",
            "Accuracy with solver 'saga': 0.8550\n",
            "Accuracy with solver 'lbfgs': 0.8550\n",
            "\n",
            "Demonstration of penalty affect on solvers:\n",
            "Accuracy with solver 'liblinear' and penalty 'l1': 0.8550\n",
            "Accuracy with solver 'saga' and penalty 'l1': 0.8600\n",
            "Solver 'lbfgs' with penalty 'l1' encountered an error: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "GpYH0LvoWYxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the MCC score\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCLN67mPWfv6",
        "outputId": "85d2a4e9-6307-4d1f-eb16-ae999f36d0f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.7172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "pElc3yLvW9Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train Logistic Regression on raw data\n",
        "logistic_regression_raw = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression_raw.fit(X_train, y_train)\n",
        "y_pred_raw = logistic_regression_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "print(f\"Accuracy on raw data: {accuracy_raw:.4f}\")\n",
        "\n",
        "# 2. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Train Logistic Regression on standardized data\n",
        "logistic_regression_scaled = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = logistic_regression_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# 4. Compare the results\n",
        "if accuracy_scaled > accuracy_raw:\n",
        "    print(\"Standardization improved accuracy.\")\n",
        "elif accuracy_scaled < accuracy_raw:\n",
        "    print(\"Standardization decreased accuracy.\")\n",
        "else:\n",
        "    print(\"Standardization did not change accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZS7MpJeXEeZ",
        "outputId": "a53210bb-0db7-46ee-ff50-b78bee9a9dca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.8550\n",
            "Accuracy on standardized data: 0.8550\n",
            "Standardization did not change accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation."
      ],
      "metadata": {
        "id": "de2T6cwnXoVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for C\n",
        "param_grid = {'C': np.logspace(-4, 4, 20)}  # Try a range of C values\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Perform grid search to find the optimal C\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best C value and best score\n",
        "best_c = grid_search.best_params_['C']\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Print the best C value and best score\n",
        "print(f\"Best C: {best_c:.4f}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_score:.4f}\")\n",
        "\n",
        "# Train the final model with the optimal C on the entire training set.\n",
        "final_logistic = LogisticRegression(C=best_c, random_state=42, max_iter=1000)\n",
        "final_logistic.fit(X_train,y_train)\n",
        "\n",
        "#Evaluate on test set.\n",
        "test_accuracy = final_logistic.score(X_test, y_test)\n",
        "print(f\"Test Accuracy with optimal C: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2HubmVxXx8R",
        "outputId": "cc284835-156a-4db1-8806-5a71753620a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 0.0886\n",
            "Best Cross-Validation Accuracy: 0.8700\n",
            "Test Accuracy with optimal C: 0.8650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###25.  Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions."
      ],
      "metadata": {
        "id": "1CI7UT9EX99o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "import joblib\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "model_filename = 'logistic_regression_model.joblib'\n",
        "joblib.dump(logistic_regression, model_filename)\n",
        "\n",
        "print(f\"Model saved as {model_filename}\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load(model_filename)\n",
        "\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "\n",
        "# Optional: Verify that the predictions are the same as the original model\n",
        "y_pred_original = logistic_regression.predict(X_test)\n",
        "\n",
        "if np.array_equal(y_pred_loaded, y_pred_original):\n",
        "    print(\"Predictions from loaded model match original model.\")\n",
        "else:\n",
        "    print(\"Predictions from loaded model do not match original model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNmbEIeuYI2c",
        "outputId": "3cca3dfd-56cf-4597-d7d9-5cf4b674b85b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as logistic_regression_model.joblib\n",
            "Model loaded successfully.\n",
            "Predictions from loaded model match original model.\n"
          ]
        }
      ]
    }
  ]
}