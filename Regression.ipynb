{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Regression\n",
        "Assignment Questions"
      ],
      "metadata": {
        "id": "BI7d6-VmHArG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. What is Simple Linear Regression ?\n",
        "Ans-1. Simple Linear Regression is a statistical method used to model the relationship between two continuous variables. It aims to find a linear equation that best describes how changes in one variable (the independent variable or predictor) are associated with changes in another variable (the dependent variable or response).\n",
        "\n",
        "####**Key Concepts:**\n",
        "\n",
        "* **Linear Relationship:** The core assumption is that the relationship between the two variables can be reasonably approximated by a straight line.\n",
        "\n",
        "* **Equation:** The simple linear regression model is represented by the equation:\n",
        "\n",
        "* y = a + bx\n",
        "* where:\n",
        "* y is the dependent variable\n",
        "* x is the independent variable\n",
        "* a is the intercept (the value of y when x is 0)\n",
        "* b is the slope (the change in y for each unit change in x)\n",
        "* **Least Squares Method:** The most common method to find the best-fitting line is the least squares method. This method minimizes the sum of the squared differences between the actual data points and the predicted values from the regression line.\n",
        "\n",
        "####**Uses of Simple Linear Regression:**\n",
        "\n",
        "* **Prediction:** Predicting the value of the dependent variable for a given value of the independent variable.\n",
        "\n",
        "* **Understanding Relationships:** Determining the strength and direction of the relationship between two variables.\n",
        "\n",
        "* **Hypothesis Testing:** Testing whether there is a statistically significant relationship between the two variables.\n",
        "\n",
        "####**Example:**\n",
        "\n",
        "Let's say you want to predict the price of a house based on its size (square footage). You can use simple linear regression to find the equation that best represents the relationship between house size and price. The slope of the line would tell you how much the price typically increases for each additional square foot of living space.\n",
        "\n",
        "####**Limitations:**\n",
        "\n",
        "* **Assumes Linearity:** Simple linear regression assumes a linear relationship between the variables. If the relationship is non-linear, the model may not accurately represent the data.\n",
        "\n",
        "* **Sensitive to Outliers:** Outliers can significantly influence the regression line, potentially leading to inaccurate predictions.\n",
        "\n",
        "* **May Not Capture Complex Relationships:** Simple linear regression can only model the relationship between two variables. If there are other factors that influence the dependent variable, they may not be captured by the model.\n",
        "\n",
        "Despite its limitations, simple linear regression is a valuable tool for understanding and analyzing relationships between two continuous variables in many fields, including economics, finance, social sciences, and engineering.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "7iczeyvjHBX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. What are the key assumptions of Simple Linear Regression ?\n",
        "Ans-2.**The key assumptions of simple linear regression are:**\n",
        "\n",
        "* **Linearity:** The relationship between the independent variable (X) and the mean of the dependent variable (Y) must be linear. This means the data points should roughly form a straight line when plotted on a scatterplot.\n",
        "\n",
        "* **Independence of errors:** The residuals (the differences between the actual Y values and the predicted Y values) should be independent of each other. This means there shouldn't be any patterns or correlations between the residuals.\n",
        "\n",
        "* **Homoscedasticity:** The variance of the residuals should be constant for all values of the independent variable (X). This means the spread of the data points around the regression line should be roughly equal across the range of X values.\n",
        "\n",
        "* **Normality of errors:** For any fixed value of X, the distribution of Y should be approximately normal. This means the residuals should be normally distributed.\n",
        "\n",
        "These assumptions are important because they ensure that the statistical inferences made from the regression model are valid and reliable. If these assumptions are violated, the results of the regression analysis may be misleading."
      ],
      "metadata": {
        "id": "wUO6hpC7c-13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. What does the coefficient m represent in the equation Y=mX+c ?\n",
        "Ans-3. **The Coefficient 'm' in y = mx + c**\n",
        "\n",
        "In the equation y = mx + c, the coefficient 'm' represents the slope or gradient of the line.\n",
        "\n",
        "####What does slope mean?\n",
        "\n",
        "It indicates how steep the line is.\n",
        "It tells us how much the value of 'y' changes for every unit change in 'x'.\n",
        "\n",
        "Here's a visual representation:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "* **Positive slope:** If 'm' is positive, the line slants upwards from left to right.A larger positive value of 'm' means a steeper upward slant.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "* **Negative slope:** If 'm' is negative, the line slants downwards from left to right. A larger absolute value of a negative 'm' means a steeper downward slant.\n",
        "\n",
        "####In summary:\n",
        "\n",
        "The sign of 'm' determines the direction of the line's slant.\n",
        "The magnitude of 'm' determines the steepness of the line.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8_mu6XXgeJfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. What does the intercept c represent in the equation Y=mX+c ?\n",
        "Ans-4. In the equation y = mx + c, the coefficient 'c' represents the y-intercept of the line.\n",
        "\n",
        "What is the y-intercept?\n",
        "* It's the point where the line crosses the y-axis.\n",
        "\n",
        "* It's the value of 'y' when 'x' is 0.\n",
        "\n",
        "####**Key points about the y-intercept:**\n",
        "\n",
        "* It's a constant value in the equation.\n",
        "\n",
        "* it determines the vertical shift of the line.\n",
        "\n",
        "* A positive 'c' shifts the line upwards, while a negative 'c' shifts it downwards.\n",
        "\n"
      ],
      "metadata": {
        "id": "dmSLPXYvjux0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. How do we calculate the slope m in Simple Linear Regression ?\n",
        "Ans-5. The slope (m) in simple linear regression represents the change in the dependent variable (y) for every unit change in the independent variable (x).\n",
        "\n",
        "**It's calculated using the following formula:**\n",
        "\n",
        "####**m = Σ((x - x̄)(y - ȳ)) / Σ(x - x̄)²**\n",
        "\n",
        "####where:\n",
        "\n",
        "* Σ represents the sum of the values\n",
        "\n",
        "* x and y are the individual data points\n",
        "\n",
        "* x̄ is the mean of the x values\n",
        "\n",
        "* ȳ is the mean of the y values\n",
        "\n",
        "####Here's a breakdown of the calculation steps:\n",
        "\n",
        "1. **Calculate the means:**\n",
        "\n",
        "* Find the mean of the x values (x̄).\n",
        "* Find the mean of the y values (ȳ).\n",
        "\n",
        "2. **Calculate the deviations:**\n",
        "\n",
        "* For each data point, subtract the mean of x from x (x - x̄).\n",
        "\n",
        "* For each data point, subtract the mean of y from y (y - ȳ).\n",
        "\n",
        "3. **Calculate the products of deviations:**\n",
        "\n",
        "* Multiply the deviations of x and y for each data point ((x - x̄)(y - ȳ)).\n",
        "\n",
        "4. **Calculate the sum of products:**\n",
        "\n",
        "* Sum up all the products of deviations (Σ((x - x̄)(y - ȳ))).\n",
        "\n",
        "5. **Calculate the squared deviations of x:**\n",
        "\n",
        "* Square the deviations of x for each data point ((x - x̄)²).\n",
        "\n",
        "6. **Calculate the sum of squared deviations:**\n",
        "\n",
        "* Sum up all the squared deviations of x (Σ(x - x̄)²).\n",
        "\n",
        "7. **Divide the sum of products by the sum of squared deviations:**\n",
        "\n",
        "* Divide the sum of products from step 4 by the sum of squared deviations from step 6. This gives you the slope (m).\n",
        "\n",
        "####Note:\n",
        "\n",
        "* The slope can be positive or negative, indicating a positive or negative relationship between x and y.\n",
        "\n",
        "* The slope can also be zero, indicating no linear relationship between x and y.\n",
        "\n",
        "By calculating the slope, we can better understand the relationship between the independent and dependent variables in a simple linear regression model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wicBE502sVpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. What is the purpose of the least squares method in Simple Linear Regression ?\n",
        "Ans-6. The purpose of the least squares method in Simple Linear Regression is to find the line of best fit that minimizes the sum of the squared differences between the actual data points and the predicted values from the regression line.\n",
        "\n",
        "####Here's a breakdown:\n",
        "\n",
        "**Minimizing Errors:**\n",
        "\n",
        "The least squares method aims to find the line that results in the smallest possible sum of the squared vertical distances between each data point and the corresponding point on the regression line. These vertical distances are called residuals.\n",
        "\n",
        "* By minimizing the sum of the squared residuals, the method ensures that the line is as close as possible to all the data points, providing the best overall fit.\n",
        "\n",
        "**Finding the Best Fit:**\n",
        "\n",
        "* The line determined by the least squares method is called the regression line or the line of best fit.\n",
        "\n",
        "* This line provides the most accurate predictions of the dependent variable (y) for given values of the independent variable (x) based on the observed data.\n",
        "\n",
        "####**Key Concept:**\n",
        "\n",
        "* The least squares method is essentially about finding the line that best represents the overall trend in the data while minimizing the overall error.\n",
        "\n",
        "In essence, the least squares method helps us find the most accurate and representative linear relationship between two variables in a simple linear regression model.\n"
      ],
      "metadata": {
        "id": "55GGp6kl1A5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression ?\n",
        "Ans-7. The coefficient of determination, denoted as R², is a statistical measure that represents the proportion of variance in the dependent variable that can be explained by the independent variable in a regression model.\n",
        "\n",
        "##**Interpretation:**\n",
        "\n",
        "* **0 to 1:** R² values range from 0 to 1.\n",
        "\n",
        "* **R² = 0:** Indicates that the independent variable does not explain any of the variability in the dependent variable. The model has no predictive power.\n",
        "\n",
        "* **R² = 1:** Indicates that the independent variable perfectly explains all the variability in the dependent variable. The model is a perfect fit for the data.\n",
        "\n",
        "* **0 < R² < 1:** Indicates that the independent variable explains a portion of the variability in the dependent variable. A higher R² value generally suggests a better fit of the model to the data.\n",
        "\n",
        "* **Percentage:** R² is often expressed as a percentage by multiplying it by 100.\n",
        "\n",
        "**For example,** an R² of 0.75 means that 75% of the variance in the dependent variable can be explained by the independent variable.\n",
        "\n",
        "###**Key Points:**\n",
        "\n",
        "* **Goodness of Fit:**  R² provides a measure of how well the regression model fits the observed data.\n",
        "\n",
        "* **Predictive Power:**  A higher R² generally indicates that the model has better predictive power.\n",
        "\n",
        "###**Limitations:**\n",
        "\n",
        "* R² can increase simply by adding more independent variables to the model, even if those variables are not meaningful.\n",
        "\n",
        "* R² does not indicate whether the model is statistically significant or whether the relationship between the variables is causal.\n",
        "\n",
        "####**In summary:**\n",
        "\n",
        "* The coefficient of determination (R²) is a valuable tool for assessing the goodness of fit of a regression model.However, it's important to interpret R² in conjunction with other model evaluation metrics and consider the context of the analysis.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "chLlraMl2VVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. What is Multiple Linear Regression ?\n",
        "Ans-8. **Multiple Linear Regression**\n",
        "\n",
        "Multiple Linear Regression is a statistical method used to model the relationship between a dependent variable and two or more independent variables. It extends the concept of simple linear regression, which only considers one independent variable, to handle more complex scenarios.\n",
        "\n",
        "####**Key Concepts:**\n",
        "\n",
        "* **Multiple Independent Variables:** The core idea is to predict the value of a dependent variable based on the combined influence of several independent variables.\n",
        "\n",
        "* **Linear Relationship:** The model assumes a linear relationship between the dependent variable and each independent variable.\n",
        "\n",
        "* **Equation:** The general equation for multiple linear regression is:\n",
        "            Y = b0 + b1X1 + b2X2 + ... + bnXn + ε\n",
        "\n",
        "            \n",
        "* Y: The dependent variable\n",
        "\n",
        "* b0: The intercept (the value of Y when all independent variables are 0)\n",
        "\n",
        "* b1, b2, ..., bn: The coefficients (slopes) representing the change in Y for a unit change in each independent variable, holding other variables constant.\n",
        "\n",
        "* X1, X2, ..., Xn: The independent variables.\n",
        "\n",
        "* ε: The error term (residuals).\n",
        "\n",
        "* **Least Squares Method:** Similar to simple linear regression, the least squares method is used to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the sum of the squared differences between the actual data points and the predicted values.\n",
        "\n",
        "####**Uses of Multiple Linear Regression:**\n",
        "\n",
        "* **Prediction:** Predicting the value of the dependent variable for given values of the independent variables.\n",
        "\n",
        "* **Understanding Relationships:** Determining the strength and direction of the relationship between the dependent variable and each independent variable, while controlling for the effects of other variables.\n",
        "\n",
        "* **Hypothesis Testing:** Testing whether there is a statistically significant relationship between the dependent variable and each independent variable.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's say you want to predict the price of a house. Instead of just considering the size (square footage) as in simple linear regression, you can include other factors like:\n",
        "\n",
        "* Number of bedrooms\n",
        "\n",
        "* Number of bathrooms\n",
        "\n",
        "* Age of the house\n",
        "\n",
        "* Location (neighborhood, proximity to amenities)\n",
        "\n",
        "Multiple linear regression would then analyze how these factors together influence the house price.\n",
        "\n",
        "####**Limitations:**\n",
        "\n",
        "* **Assumes Linearity:** As with simple linear regression, it assumes a linear relationship between the dependent variable and each independent variable.\n",
        "\n",
        "* **Sensitive to Outliers:** Outliers can significantly influence the regression model.\n",
        "\n",
        "* **Multicollinearity:** If the independent variables are highly correlated with each other (multicollinearity), it can make it difficult to accurately estimate the individual effects of each variable.\n",
        "\n",
        "* **Overfitting:** Including too many independent variables can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
        "\n",
        "Despite these limitations, multiple linear regression is a powerful tool for analyzing complex relationships between variables in various fields, including economics, finance, social sciences, and engineering.\n",
        "\n"
      ],
      "metadata": {
        "id": "B0AHpQCV5EX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9. What is the main difference between Simple and Multiple Linear Regression ?\n",
        "Ans-9. The main difference between Simple and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable:\n",
        "\n",
        "##**Simple Linear Regression:**\n",
        "\n",
        "* Uses one independent variable to predict the dependent variable.\n",
        "\n",
        "* Models the relationship between two variables using a straight line.\n",
        "\n",
        "* Equation: The relationship is represented by\n",
        "    𝑌 = 𝑚x + c\n",
        "* where\n",
        "𝑌\n",
        " is the dependent variable,\n",
        "𝑋\n",
        " is the independent variable,\n",
        "𝑚\n",
        " is the slope, and\n",
        "𝑐\n",
        " is the intercept.\n",
        "\n",
        "##**Multiple Linear Regression:**\n",
        "\n",
        "* Uses two or more independent variables to predict the dependent variable.\n",
        "\n",
        "* Models the relationship between the dependent variable and a combination of multiple independent variables.\n",
        "\n",
        "* Equation: The relationship is represented by\n",
        "    Y = b0 + b1X1 + b2X2 + ... + bnXn + ε\n",
        "\n",
        "* where\n",
        "𝑌\n",
        " is the dependent variable,\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        " are the independent variables,\n",
        "𝑏\n",
        "0\n",
        " is the intercept, and\n",
        "𝑏\n",
        "1\n",
        ",\n",
        "𝑏\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑏\n",
        "𝑛\n",
        " are the coefficients (slopes) for each independent variable.\n",
        "\n",
        "#### **In essence,** simple linear regression examines the impact of one variable on the outcome, while multiple linear regression assesses the influence of several variables on the outcome.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZdnU700L8vMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10. What are the key assumptions of Multiple Linear Regression ?\n",
        "Ans-10. Multiple Linear Regression, like its simpler counterpart, relies on several key assumptions for valid and reliable results.\n",
        "These assumptions ensure that the model accurately represents the relationships between the variables and that the statistical inferences drawn from the model are meaningful. Here are the primary assumptions:\n",
        "\n",
        "1.**Linearity:**\n",
        "\n",
        "* The relationship between the dependent variable and each independent variable is assumed to be linear.\n",
        "\n",
        "* This means that the change in the dependent variable is directly proportional to changes in the independent variables.\n",
        "\n",
        "* Violations of linearity can lead to biased and inefficient estimates of the coefficients.\n",
        "\n",
        "2.**No Multicollinearity:**\n",
        "\n",
        "* Multicollinearity occurs when two or more independent variables are highly correlated with each other.\n",
        "\n",
        "* This can make it difficult to determine the unique contribution of each independent variable to the model, as it becomes challenging to isolate their individual effects.\n",
        "\n",
        "* High multicollinearity can lead to unstable and unreliable coefficient estimates.\n",
        "\n",
        "3.Homoscedasticity:\n",
        "\n",
        "* This assumption states that the variance of the errors (residuals) is constant across all levels of the independent variables.\n",
        "\n",
        "* In other words, the spread of the data points around the regression line should be roughly equal for all values of the independent variables.\n",
        "\n",
        "* Heteroscedasticity (non-constant variance) can lead to inefficient and biased coefficient estimates.\n",
        "\n",
        "4.**Independence of Errors:**\n",
        "\n",
        "* The errors (residuals) are assumed to be independent of each other.\n",
        "\n",
        "* This means that the error in one observation does not influence the error in another observation.\n",
        "\n",
        "* Violations of independence can occur in time series data or when observations are clustered.\n",
        "\n",
        "5.**Normality of Errors:**\n",
        "\n",
        "* The errors (residuals) are assumed to be normally distributed.\n",
        "\n",
        "* This assumption is important for hypothesis testing and constructing confidence intervals for the coefficients.\n",
        "\n",
        "####Consequences of Violated Assumptions:\n",
        "\n",
        "* Biased and inefficient coefficient estimates: Incorrect inferences about the relationships between variables.\n",
        "\n",
        "* Invalid hypothesis tests: Incorrect conclusions about the statistical significance of the model and individual coefficients.\n",
        "\n",
        "* Poor model performance: Inaccurate predictions of the dependent variable.\n",
        "\n",
        "####**Checking Assumptions:**\n",
        "\n",
        "**Linearity:** Scatter plots, residual plots\n",
        "\n",
        "**Multicollinearity:** Variance Inflation Factor (VIF), correlation matrix\n",
        "\n",
        "**Homoscedasticity:** Residual plots, Breusch-Pagan test\n",
        "\n",
        "**Independence:** Durbin-Watson test, time series plots\n",
        "\n",
        "**Normality:** Histogram of residuals, Q-Q plot\n",
        "\n",
        "By carefully checking these assumptions and addressing any violations, you can ensure the validity and reliability of your multiple linear regression model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdeB1op3BNGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model ?\n",
        "Ans-11. **Heteroscedasticity** in multiple linear regression refers to the situation where the variance of the residuals (the differences between the actual values and the predicted values) is not constant across all levels of the independent variables.In simpler terms, the spread of the data points around the regression line is not uniform.\n",
        "\n",
        "####**How it Affects Multiple Linear Regression Results:**\n",
        "\n",
        "1. **Inaccurate Standard Errors:** Heteroscedasticity leads to incorrect estimates of the standard errors of the regression coefficients.Standard errors are crucial for hypothesis testing and constructing confidence intervals. If they are inaccurate, the statistical significance of the coefficients and the reliability of the model's predictions can be compromised.\n",
        "\n",
        "2. **Inefficient Coefficient Estimates:** The least squares method, which is commonly used to estimate the coefficients in multiple linear regression, assumes homoscedasticity (constant variance of residuals). When heteroscedasticity is present, the least squares estimates may not be the most efficient, meaning there might be other estimators that provide more accurate results.\n",
        "\n",
        "3. **Incorrect Inferences:** Inaccurate standard errors can lead to incorrect conclusions about the statistical significance of the coefficients. You might incorrectly reject or fail to reject the null hypothesis, leading to misleading interpretations of the relationships between the variables.\n",
        "\n",
        "####**Visualizing Heteroscedasticity:**\n",
        "\n",
        "A common way to detect heteroscedasticity is by examining a plot of the residuals against the fitted values (predicted values). If the plot shows a pattern, such as a funnel shape or a cone shape, it suggests the presence of heteroscedasticity.\n",
        "\n",
        "####**Addressing Heteroscedasticity:**\n",
        "\n",
        "Several techniques can be used to address heteroscedasticity:\n",
        "\n",
        "* **Transformations:** Transforming the dependent variable or independent variables (e.g., taking the logarithm or square root) can sometimes stabilize the variance.\n",
        "\n",
        "* **Weighted Least Squares:** This method assigns different weights to observations based on their variance, giving more weight to observations with smaller variances.\n",
        "\n",
        "* **Robust Regression Methods:** These methods are less sensitive to heteroscedasticity and can provide more reliable estimates.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "Heteroscedasticity is a violation of the assumptions of multiple linear regression that can have serious consequences for the validity and reliability of the model's results.It's essential to check for heteroscedasticity and address it appropriately to ensure accurate and meaningful conclusions.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h2LCf1nyEee3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###12. How can you improve a Multiple Linear Regression model with high multicollinearity ?\n",
        "Ans-12.  \n",
        "\n",
        "1.**Identify and Remove Highly Correlated Predictors**\n",
        "\n",
        "* **Calculate Variance Inflation Factor (VIF):** VIF measures how much the variance of an estimated regression coefficient is inflated due to collinearity with other predictors.\n",
        "\n",
        "* A general rule of thumb is that a VIF greater than 5 or 10 indicates high multicollinearity.\n",
        "\n",
        "* **Remove Predictors:** Remove one of the highly correlated predictors. You can use domain knowledge or iterative model building to determine which predictor to remove.\n",
        "\n",
        "2.**Combine Correlated Predictors**\n",
        "\n",
        "* **Create a new variable:** Combine highly correlated predictors into a single, more meaningful predictor. For example, if \"years of experience\" and \"years of education\" are highly correlated, you could create a new variable called \"professional experience.\"\n",
        "\n",
        "* Principal Component Analysis (PCA): PCA transforms the original set of correlated predictors into a new set of uncorrelated components. These components can then be used as predictors in the regression model.\n",
        "\n",
        "3.**Regularization Techniques**\n",
        "\n",
        "* **Ridge Regression:** Adds a penalty term to the regression equation that shrinks the coefficients of correlated predictors, reducing their influence.\n",
        "\n",
        "* **Lasso Regression:** Similar to Ridge Regression, but it can also set the coefficients of some predictors to zero, effectively removing them from the model.\n",
        "\n",
        "4.**Increase Sample Size**\n",
        "\n",
        "* With a larger sample size, the effects of multicollinearity can be mitigated as the model has more data to learn from and can better distinguish between the effects of correlated predictors.\n",
        "\n",
        "5.**Centering the Data**\n",
        "\n",
        "* Centering the data (subtracting the mean from each variable) can sometimes reduce the impact of multicollinearity, especially when interaction terms are involved.\n",
        "\n",
        "**Important Considerations:**\n",
        "\n",
        "* **Domain Knowledge:** Use your understanding of the data and the underlying relationships between the variables to guide your decisions.\n",
        "\n",
        "* **Trade-offs:** Removing predictors or combining them may lead to a loss of information.\n",
        "\n",
        "* **Model Interpretation:** If interpretability is crucial, some methods (like PCA) may make it more challenging to understand the individual contributions of the original predictors.\n",
        "\n",
        "By carefully considering these approaches and evaluating their impact on model performance, we can effectively address multicollinearity and improve the reliability and interpretability of your multiple linear regression model.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Ft1_8kLFadT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13. What are some common techniques for transforming categorical variables for use in regression models ?\n",
        "Ans-13. Transforming categorical variables for regression models is essential because these models typically require numerical input. Here are some common techniques:\n",
        "\n",
        "1. **One-Hot Encoding:** Converts each category into a new binary column (0 or 1). Useful when there are a few distinct categories.\n",
        "\n",
        "2. **Label Encoding:** Assigns each category a unique integer value. This can sometimes introduce ordinality, which may or may not be desirable.\n",
        "\n",
        "3. **Ordinal Encoding:** Similar to label encoding but used when the categories have a meaningful order.\n",
        "\n",
        "4. **Binary Encoding:** Each category is converted to binary code, and each digit of the binary code becomes a separate column.\n",
        "\n",
        "5. **Frequency Encoding:** Encodes categories based on the frequency of their occurrence in the dataset. Useful for high-cardinality categorical variables.\n",
        "\n",
        "6. **Target Encoding:** Replaces categories with the mean of the target variable for each category. It can sometimes help capture relationships between categorical variables and the target.\n",
        "\n",
        "7. **Count Encoding:** Replaces each category with the count of the number of times it appears in the dataset. This can be particularly useful for high-cardinality variables.\n",
        "\n",
        "8. **Feature Hashing:** Uses a hash function to convert categories into numerical values. It can handle high-cardinality variables efficiently but can sometimes lead to collisions (two categories being hashed to the same value).\n",
        "\n",
        "####**Choosing the Right Technique:**\n",
        "\n",
        "The best encoding technique depends on the specific characteristics of the categorical variable, the nature of the data, and the chosen regression model. Consider factors like:\n",
        "\n",
        "* **Cardinality:** The number of unique categories in the variable.\n",
        "Order: Whether the categories have an inherent order.\n",
        "\n",
        "* **Model complexity:** Some models (e.g., tree-based models) can handle categorical variables directly without explicit encoding.\n",
        "\n",
        "By carefully selecting and applying the appropriate encoding technique, we can effectively incorporate categorical variables into your regression models and improve their predictive performance."
      ],
      "metadata": {
        "id": "E-8EgXXpFcOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14. What is the role of interaction terms in Multiple Linear Regression ?\n",
        "Ans-14. Interaction terms in Multiple Linear Regression (MLR) allow us to explore how the relationship between one predictor variable and the response variable changes depending on the level of another predictor variable. In other words, interaction terms help us understand if the effect of one variable on the outcome is influenced by the value of another variable.\n",
        "\n",
        "**Here's a simplified example:**\n",
        "\n",
        "Suppose we have two predictor variables,\n",
        "𝑋\n",
        "1\n",
        " (say, hours of study) and\n",
        "𝑋\n",
        "2\n",
        " (say, attendance). Without interaction, the model would look something like this:\n",
        "\n",
        " 𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝜖\n",
        "\n",
        "where:\n",
        "\n",
        "𝑌\n",
        " is the outcome (e.g., exam scores),\n",
        "\n",
        "𝛽\n",
        "0\n",
        " is the intercept,\n",
        "\n",
        "𝛽\n",
        "1\n",
        " and\n",
        "𝛽\n",
        "2\n",
        " are the coefficients,\n",
        "\n",
        "𝜖\n",
        " is the error term.\n",
        "\n",
        "With an interaction term, we introduce a term\n",
        "𝑋\n",
        "1\n",
        "×\n",
        "𝑋\n",
        "2\n",
        ", and the model becomes:\n",
        "\n",
        "   𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "(\n",
        "𝑋\n",
        "1\n",
        "×\n",
        "𝑋\n",
        "2\n",
        ")\n",
        "+\n",
        "𝜖\n",
        "\n",
        "Here,\n",
        "𝛽\n",
        "3\n",
        " is the coefficient for the interaction term. This new term allows us to capture how the effect of hours of study on exam scores might depend on attendance.\n",
        "\n",
        "####**Benefits of including interaction terms:**\n",
        "\n",
        "1. Capture Complexity: Real-world relationships are often more complex than simple additive effects.\n",
        "\n",
        "2. Improve Model Fit: Including interaction terms can improve the fit of the model to the data, leading to better predictions.\n",
        "\n",
        "3. Insights: It provides deeper insights into the relationships between variables.\n",
        "\n",
        "**Considerations:**\n",
        "\n",
        "Interpretability: Models with many interaction terms can become harder to interpret.\n",
        "\n",
        "Overfitting: Adding too many interaction terms can lead to overfitting, especially with small datasets.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "Interaction terms are a valuable tool in multiple linear regression for capturing complex relationships between variables. By carefully considering the potential for interactions and including them in our models, we can gain a deeper understanding of the factors that influence your dependent variable and improve the accuracy of your predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "zhz4wj58MsE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "Ans-15.\n",
        "##**In Simple Linear Regression:**\n",
        "\n",
        "* **Clear Interpretation:** The intercept represents the predicted value of the dependent variable (Y) when the independent variable (X) is equal to zero.\n",
        "\n",
        "* **Example:** If predicting house prices based on size, the intercept would be the predicted price of a house with zero square footage (which is obviously not realistic, but it's the mathematical interpretation).\n",
        "\n",
        "##**In Multiple Linear Regression:**\n",
        "\n",
        "**More Complex Interpretation:**\n",
        "\n",
        "* The intercept represents the predicted value of the dependent variable when all independent variables are equal to zero.\n",
        "\n",
        "* This can often be a less meaningful interpretation, especially when:\n",
        "Some or all independent variables cannot realistically be zero.\n",
        "\n",
        "* The combination of all independent variables being zero is outside the range of the observed data.\n",
        "\n",
        "####**Key Differences:**\n",
        "\n",
        "* **Number of Variables:** In simple regression, the intercept relates to a single independent variable being zero. In multiple regression, it relates to all independent variables being zero simultaneously.\n",
        "\n",
        "* **Real-World Applicability:** The intercept in simple regression might have a more direct real-world interpretation, while in multiple regression, it might be more of a mathematical construct with limited practical significance.\n",
        "\n",
        "**Important Considerations:**\n",
        "\n",
        "* **Data Range:** If the observed data for the independent variables never includes values close to zero, the intercept may have little practical meaning in either case.\n",
        "\n",
        "* **Centering Variables:** Centering the independent variables (subtracting the mean from each value) can sometimes make the intercept more interpretable, as it then represents the predicted value of the dependent variable when the independent variables are at their mean values.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "While the concept of the intercept remains the same (the predicted value when all predictors are zero), its interpretation and practical significance can differ significantly between simple and multiple linear regression due to the increased complexity of the model with multiple predictors."
      ],
      "metadata": {
        "id": "_4hyqFHbOkx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16. What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
        "Ans-16. **Significance of the Slope in Regression Analysis**\n",
        "\n",
        "In regression analysis, the slope is a crucial parameter that quantifies the relationship between the independent variable (X) and the dependent variable (Y). It represents the rate of change in Y for a unit change in X.\n",
        "\n",
        "####**Key Significance:**\n",
        "\n",
        "**Direction of Relationship:**\n",
        "\n",
        "* **Positive Slope:** Indicates a positive relationship, meaning that as X increases, Y tends to increase as well.\n",
        "\n",
        "* **Negative Slope:** Suggests an inverse relationship, where an increase in X is associated with a decrease in Y.\n",
        "\n",
        "* **Zero Slope:** Implies no linear relationship between X and Y.\n",
        "\n",
        "* **Strength of Relationship:** The magnitude of the slope provides an indication of the strength of the linear relationship. A steeper slope generally signifies a stronger relationship.\n",
        "\n",
        "####**Impact on Predictions**\n",
        "\n",
        "The slope directly influences the predictions made by the regression model. Here's how:\n",
        "\n",
        "Prediction Equation: The regression equation typically takes the form:\n",
        "\n",
        "              Y = b0 + b1*X\n",
        "\n",
        "where:\n",
        "\n",
        "* Y is the predicted value of the dependent variable\n",
        "\n",
        "* b0 is the intercept (the value of Y when X is 0)\n",
        "\n",
        "* b1 is the slope\n",
        "\n",
        "* X is the value of the independent variable\n",
        "\n",
        "* Change in Predictions: For a unit change in X, the predicted value of Y changes by the value of the slope (b1).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's say you have a regression model predicting house prices based on square footage. If the slope is 100, it means that for every additional square foot, the predicted house price increases by $100.\n",
        "\n",
        "**In essence,** the slope in regression analysis is a vital component that determines the nature and strength of the relationship between variables, ultimately driving the accuracy and reliability of the predictions generated by the model."
      ],
      "metadata": {
        "id": "1_yIwg9KRlsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17. How does the intercept in a regression model provide context for the relationship between variables ?\n",
        "Ans-17. The intercept in a regression model provides valuable context for the relationship between variables by representing the predicted value of the dependent variable (Y) when all independent variables (X) are equal to zero. Here's how it provides context:\n",
        "\n",
        "1. **Baseline Value:**\n",
        "\n",
        "* **Starting Point:** The intercept serves as the starting point or baseline for the relationship. It indicates the expected value of Y when the influence of all independent variables is absent.\n",
        "\n",
        "* **Meaningful Interpretation:** If the values of independent variables can realistically be zero within the context of the study, the intercept has a direct interpretation.\n",
        "\n",
        "* **Sales Prediction:** If the intercept in a sales prediction model represents sales when all marketing efforts are zero, it provides a baseline for understanding the impact of marketing activities on sales.\n",
        "\n",
        "* **Cost Analysis:** In a cost analysis model, the intercept might represent fixed costs that are incurred regardless of the level of production or other activities.\n",
        "\n",
        "2. **Relationship Interpretation:**\n",
        "\n",
        "* **Positive Intercept:** A positive intercept suggests that even in the absence of the influence of independent variables, there's a positive baseline value for the dependent variable.\n",
        "\n",
        "* **Negative Intercept:** A negative intercept implies that the predicted value of Y is negative when all independent variables are zero. This might be meaningful in certain contexts but requires careful interpretation.\n",
        "\n",
        "* **Zero Intercept:** A zero intercept indicates that the regression line passes through the origin, suggesting that when all independent variables are zero, the predicted value of the dependent variable is also zero.\n",
        "\n",
        "3. **Limitations:**\n",
        "\n",
        "* **Meaningless Interpretation:** In some cases, the intercept might not have a meaningful interpretation. For example, if the independent variable represents a physical quantity that cannot realistically be zero (e.g., weight, temperature), the intercept might not provide useful information.\n",
        "\n",
        "* **Extrapolation:** It's important to avoid extrapolating beyond the range of the data. Interpreting the intercept as the predicted value when independent variables are zero might not be valid if the model is not reliable outside the observed data range.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "The intercept in a regression model provides valuable context by establishing a baseline for the relationship between variables.It helps in understanding the expected value of the dependent variable when the influence of independent variables is minimal or absent. However, it's crucial to consider the context and limitations of the model before interpreting the intercept.\n"
      ],
      "metadata": {
        "id": "cP1ET8QSCNtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###18. What are the limitations of using R² as a sole measure of model performance ?\n",
        "Ans-18. **R-squared (R²)** is a commonly used metric in regression analysis to assess the goodness-of-fit of a model. It measures the proportion of the variance in the dependent variable that is predictable from the independent variables. While R² can be useful in determining the strength of the relationship between variables, it has several limitations when used as the sole measure of model performance:\n",
        "\n",
        "1. **Sensitivity to Outliers:** R² can be heavily influenced by outliers in the data. A few extreme observations can disproportionately affect the R² value, making it difficult to draw accurate conclusions about the correlation.\n",
        "\n",
        "2. **Correlation ≠ Causation:** A high R² value indicates a strong relationship between the independent and dependent variables, but it does not imply causation. It is crucial to remember that correlation does not equal causation, and further investigation is needed to determine if a causal relationship exists between the variables.\n",
        "\n",
        "3. **Overfitting:** A high R² value might result from overfitting the model to the data, particularly when there are many independent variables or when the sample size is small. In such cases, the model may capture noise in the data rather than the true underlying relationship, leading to misleading conclusions.\n",
        "\n",
        "4. **R² Increases with More Independent Variables:** R² value will never decrease when adding more independent variables to the model, even if they have little to no correlation with the dependent variable. This can give a false impression of improved model performance when, in reality, the added variables may not be meaningful.\n",
        "\n",
        "5. **Scale Dependency:** R² is sensitive to the scale of the variables, which may affect its interpretation. When comparing models with different units or scales, R² values may not be directly comparable.\n",
        "\n",
        "####To address these limitations, it's important to consider other evaluation metrics alongside R², such as:\n",
        "\n",
        "* **Adjusted R²:** This metric takes into account the number of independent variables and sample size, providing a more accurate measure of model fit.\n",
        "\n",
        "* **Mean Squared Error (MSE):** This metric measures the average squared difference between the predicted and actual values.\n",
        "\n",
        "* **Root Mean Squared Error (RMSE):** This metric is the square root of MSE and provides a measure of the average prediction error.\n",
        "\n",
        "* **Mean Absolute Error (MAE):** This metric measures the average absolute difference between the predicted and actual values.\n",
        "\n",
        "* By using a combination of these metrics and considering the context of the data and research question, we can gain a more comprehensive understanding of model performance."
      ],
      "metadata": {
        "id": "F9A2EuTrEJFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###19. How would you interpret a large standard error for a regression coefficient ?\n",
        "Ans-19. A large standard error for a regression coefficient indicates a high level of uncertainty about the estimate of the coefficient. In other words, it suggests that the estimated coefficient value could vary significantly if the model were to be applied to different samples of data. Here are a few implications of a large standard error:\n",
        "\n",
        "1. **Low Precision:** The coefficient estimate is imprecise, meaning the true population parameter could be quite different from the estimated value.\n",
        "\n",
        "2. **Wide Confidence Interval:** A large standard error leads to a wide confidence interval, reflecting greater uncertainty about the coefficient's true value.\n",
        "\n",
        "3. **Potential Multicollinearity:** High standard errors can be a sign of multicollinearity, where two or more predictor variables in the model are highly correlated, causing instability in the coefficient estimates.\n",
        "\n",
        "4. **Small Sample Size:** It can also result from a small sample size, which provides less information about the relationship between the predictor and the response variable.\n",
        "\n",
        "5. **High Variability:** Large standard errors can indicate high variability in the data, which makes it harder to pinpoint the exact effect of the predictor variable.\n",
        "\n",
        "**In practice, if you encounter a large standard error, you might want to:**\n",
        "\n",
        "* Check for multicollinearity using variance inflation factors (VIF).\n",
        "\n",
        "* Consider collecting more data to increase the sample size.\n",
        "\n",
        "* Re-examine the model specification and the choice of predictors.\n",
        "\n",
        "* **In essence,** large standard errors highlight the need for careful model evaluation and possibly additional data or model adjustments to obtain more reliable estimates."
      ],
      "metadata": {
        "id": "di99GkoYFtvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###20. How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "Ans-20. **Identifying Heteroscedasticity in Residual Plots**\n",
        "\n",
        "Heteroscedasticity, or unequal variance of residuals, can be visually identified in residual plots by observing patterns in the scatter of residuals.\n",
        "\n",
        "####Here are some common visual cues:\n",
        "\n",
        "* **Cone Shape:** A classic indicator is a \"cone\" or \"fan\" shape, where the spread of residuals increases or decreases as the fitted values (predicted values) increase.\n",
        "\n",
        "* **Increasing/Decreasing Spread:** The residuals might exhibit a pattern where their spread widens or narrows systematically along the x-axis (fitted values).\n",
        "\n",
        "* **Clusters of High/Low Variance:** You might notice clusters of points with high variance interspersed with clusters of points with low variance.\n",
        "\n",
        "**Why Address Heteroscedasticity?**\n",
        "\n",
        "Heteroscedasticity can have several negative consequences for regression analysis:\n",
        "\n",
        "* **Inaccurate Standard Errors:** The standard errors of the regression coefficients are typically underestimated in the presence of heteroscedasticity. This can lead to inflated t-statistics and incorrect inferences about the significance of the coefficients.\n",
        "\n",
        "* **Inefficient Estimates:** The least squares estimates of the regression coefficients might not be as efficient as they could be, meaning they might not be the most precise estimates possible.\n",
        "\n",
        "* **Misleading Hypothesis Tests:** Incorrect standard errors can lead to incorrect conclusions from hypothesis tests, potentially leading to false rejections or incorrect acceptances of null hypotheses.\n",
        "\n",
        "####Addressing Heteroscedasticity\n",
        "\n",
        "Several techniques can be used to address heteroscedasticity:\n",
        "\n",
        "* Transformations: Transforming the dependent variable (e.g., taking the logarithm) or independent variables can sometimes stabilize the variance.\n",
        "\n",
        "* Weighted Least Squares (WLS): This method assigns weights to observations based on their variance, giving more weight to observations with lower variance.\n",
        "\n",
        "* Robust Standard Errors: These methods provide more accurate standard errors even in the presence of heteroscedasticity.\n",
        "\n",
        "By carefully examining residual plots and addressing heteroscedasticity, we can improve the accuracy and reliability of our regression analysis.  "
      ],
      "metadata": {
        "id": "WH6YeOzTH-Y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R² ?\n",
        "Ans-21. **Understanding the Scenario**\n",
        "\n",
        "* **High R-squared:** This indicates that a large proportion of the variance in the dependent variable is explained by the independent variables in the model.\n",
        "\n",
        "* **Low Adjusted R-squared:** This suggests that the inclusion of additional independent variables in the model might not be significantly improving its predictive power.\n",
        "\n",
        "##Possible Reasons\n",
        "\n",
        "* Overfitting: The model might be overfitting the data. This means it's capturing noise and random fluctuations in the data rather than the true underlying relationships. Adding more variables can exacerbate this issue.\n",
        "\n",
        "* Irrelevant Predictors: Some of the independent variables in the model might not have a strong relationship with the dependent variable. These irrelevant predictors can increase the R-squared value but decrease the adjusted R-squared.\n",
        "\n",
        "* Multicollinearity: High multicollinearity (strong correlations between independent variables) can inflate the R-squared value while inflating the standard errors of the coefficients, making it difficult to determine the true impact of individual predictors.\n",
        "\n",
        "##Implications\n",
        "\n",
        "* Model Performance: A low adjusted R-squared despite a high R-squared suggests that the model might not generalize well to new, unseen data.\n",
        "\n",
        "* Model Selection: It's crucial to carefully consider the inclusion of additional variables. Adding more variables might not always improve the model's predictive accuracy and could even lead to overfitting.\n",
        "\n",
        "##Next Steps\n",
        "\n",
        "* Variable Selection: Consider techniques like:\n",
        "\n",
        "* Stepwise regression: A method for selecting the best subset of predictors.\n",
        "Regularization methods (e.g., Lasso, Ridge): These methods can help to shrink the coefficients of less important variables, potentially improving model performance and reducing overfitting.\n",
        "\n",
        "* Check for Multicollinearity: Use techniques like variance inflation factors (VIF) to identify and address multicollinearity.\n",
        "\n",
        "* Consider Model Simplification: If possible, try simplifying the model by removing less important predictors.\n",
        "\n",
        "**In essence,** while a high R-squared might seem desirable, it's essential to consider the adjusted R-squared to get a more accurate picture of the model's performance and avoid overfitting."
      ],
      "metadata": {
        "id": "0jtn9u8ZNfqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###22. Why is it important to scale variables in Multiple Linear Regression ?\n",
        "Ans-22. Scaling variables in multiple linear regression is important for several reasons:\n",
        "\n",
        "1. **Improved Gradient Descent Convergence:**\n",
        "\n",
        "* Gradient descent algorithms, often used to find the optimal model parameters, can converge more quickly and reliably when features are on a similar scale.\n",
        "\n",
        "* Features with larger scales can dominate the gradient descent process, leading to slow convergence and potential instability.\n",
        "\n",
        "2. **Better Regularization Performance:**\n",
        "\n",
        "* Regularization techniques like Ridge and Lasso regression add penalties to the model's coefficients to prevent overfitting.\n",
        "\n",
        "* Scaling ensures that these penalties are applied fairly across features, preventing features with larger scales from being disproportionately affected.\n",
        "\n",
        "3. **Enhanced Interpretability:**\n",
        "\n",
        "* When features are on a similar scale, the magnitude of the coefficients can be more easily compared and interpreted.\n",
        "\n",
        "* This allows you to understand which features have the strongest influence on the target variable.\n",
        "\n",
        "4. **Improved Numerical Stability:**\n",
        "\n",
        "* Scaling can help prevent numerical issues that can arise when features have vastly different scales.\n",
        "\n",
        "* This is particularly important when dealing with large datasets or features with very large or very small values.\n",
        "\n",
        "####**Common Scaling Methods:**\n",
        "\n",
        "Standardization (Z-score normalization):\n",
        "\n",
        "* Transforms each feature to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "* Formula: (x - mean(x)) / std(x)\n",
        "\n",
        "####**Normalization (Min-Max scaling):**\n",
        "\n",
        "* Scales each feature to a specific range, typically between 0 and 1.\n",
        "\n",
        "* Formula: (x - min(x)) / (max(x) - min(x))\n",
        "\n",
        "**When to Scale:**\n",
        "\n",
        "* Gradient descent-based optimization: Always recommended.\n",
        "\n",
        "* Regularization techniques: Essential for proper regularization.\n",
        "\n",
        "* Distance-based algorithms: Important for algorithms like K-Nearest Neighbors (KNN) and Support Vector Machines (SVM).\n",
        "\n",
        "* Interpretability and numerical stability: Beneficial in most cases.\n",
        "\n",
        "**When Not to Scale:**\n",
        "\n",
        "* Tree-based models: Decision trees and ensemble methods like Random Forest and Gradient Boosting are generally not sensitive to feature scaling.\n",
        "\n",
        "* When interpretability in original units is crucial: If you need to interpret the coefficients directly in terms of the original units of the features, scaling may not be necessary.\n",
        "\n",
        "* **In summary,** scaling variables in multiple linear regression is a crucial preprocessing step that can significantly improve model performance, stability, and interpretability, especially when using gradient descent, regularization, or distance-based algorithms."
      ],
      "metadata": {
        "id": "m873SK63PE9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###23. What is polynomial regression ?\n",
        "Ans-23. **Polynomial regression** is a type of regression analysis where the relationship between the independent variable\n",
        "𝑥\n",
        " and the dependent variable\n",
        "𝑦\n",
        " is modeled as an nth-degree polynomial. It's an extension of simple linear regression, which models the relationship as a straight line, by allowing for more complex, curved relationships.\n",
        "\n",
        " ####**Why use Polynomial Regression?**\n",
        "\n",
        "* Non-linear Relationships: Many real-world phenomena aren't linear. Polynomial regression allows you to model these complex, curved relationships between variables more accurately.\n",
        "\n",
        "* Flexibility: By increasing the degree of the polynomial, you can create more complex curves to fit intricate data patterns.\n",
        "\n",
        "####**How does it work?**\n",
        "\n",
        "* Transforming the Data: Instead of using the original independent variable (x), you create new features by raising x to different powers (x², x³, ...).\n",
        "\n",
        "* Linear Regression: You then apply linear regression to these transformed features. This essentially fits a polynomial curve to your data.\n",
        "\n",
        "Example:\n",
        "\n",
        "A simple linear regression model might look like this:\n",
        "\n",
        "    y = b0 + b1*x\n",
        "\n",
        "A quadratic (degree 2) polynomial regression model would look like this:\n",
        "\n",
        "    y = b0 + b1*x + b2*x²\n",
        "\n",
        "####Key Considerations:\n",
        "\n",
        "* **Overfitting:** Using a high-degree polynomial can lead to overfitting, where the model fits the training data too closely and performs poorly on new data.\n",
        "\n",
        "* **Model Selection:** Choosing the appropriate degree of the polynomial is crucial. Techniques like cross-validation can help determine the optimal degree.\n",
        "\n",
        "####When to use Polynomial Regression:\n",
        "\n",
        "* When the relationship between the variables appears to be non-linear.\n",
        "\n",
        "* When you need to capture complex patterns in the data.\n",
        "\n",
        "**In essence,** polynomial regression is a powerful tool for modeling non-linear relationships between variables. By carefully selecting the degree of the polynomial, you can create flexible models that accurately capture the underlying patterns in your data.\n"
      ],
      "metadata": {
        "id": "1-tQl1zaQ6YX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24. How does polynomial regression differ from linear regression ?\n",
        "Ans-24. Here are the key differences between polynomial regression and linear regression:\n",
        "\n",
        "##**Linear Regression**\n",
        "\n",
        "* **Model Form:** Represents the relationship between the dependent variable\n",
        "𝑦\n",
        " and one or more independent variables\n",
        "𝑥\n",
        " as a straight line: $$ y = \\beta_0 + \\beta_1 x + \\epsilon $$ where\n",
        "𝛽\n",
        "0\n",
        " is the intercept,\n",
        "𝛽\n",
        "1\n",
        " is the slope, and\n",
        "𝜖\n",
        " is the error term.\n",
        "\n",
        "* **Assumption:** Assumes a linear relationship between the independent and dependent variables.\n",
        "\n",
        "* **Complexity:** Simpler to implement and interpret.\n",
        "\n",
        "* **Applications:** Used when the relationship between variables is approximately linear.\n",
        "\n",
        "##**Polynomial Regression**\n",
        "\n",
        "* **Model Form:** Extends linear regression by including polynomial terms of the independent variable: $$ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\ldots + \\beta_n x^n + \\epsilon $$ where\n",
        "𝛽\n",
        "0\n",
        ",\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        " are the coefficients.\n",
        "\n",
        "* **Assumption:** Assumes a non-linear relationship that can be modeled by a polynomial curve.\n",
        "\n",
        "* **Complexity:** More complex due to additional polynomial terms. Requires careful selection of the polynomial degree to avoid overfitting.\n",
        "\n",
        "* **Applications:** Used when the data suggests a non-linear relationship that a linear model cannot capture.\n",
        "\n",
        "\n",
        "##Differences\n",
        "\n",
        "1. Relationship:\n",
        "\n",
        "* **Linear Regression:** Models a straight-line relationship.\n",
        "\n",
        "* **Polynomial Regression:** Models a curved, polynomial relationship.\n",
        "\n",
        "2. Flexibility:\n",
        "\n",
        "* **Linear Regression:** Limited to linear relationships.\n",
        "\n",
        "* **Polynomial Regression:** More flexible, can model more complex relationships.\n",
        "\n",
        "3. Model Complexity:\n",
        "\n",
        "* **Linear Regression:** Simple model with fewer parameters.\n",
        "\n",
        "* **Polynomial Regression:** More complex with additional polynomial terms, requiring more parameters.\n",
        "\n",
        "4. Overfitting:\n",
        "\n",
        "* **Linear Regression:** Less prone to overfitting with fewer parameters.\n",
        "\n",
        "**Polynomial Regression:** Higher risk of overfitting with higher-degree polynomials.\n",
        "\n",
        "5. Interpretability:\n",
        "\n",
        "* **Linear Regression:** Easier to interpret the relationship between variables.\n",
        "\n",
        "* **Polynomial Regression:** Interpretation becomes more complex with higher-degree terms.\n",
        "\n",
        "**In essence,** while linear regression is great for simple, linear relationships, polynomial regression offers a powerful tool for capturing more complex, non-linear patterns in the data. However, it comes with added complexity and potential risks of overfitting."
      ],
      "metadata": {
        "id": "a17FY25pWQPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###25. When is polynomial regression used ?\n",
        "Ans-25. **Polynomial regression** is particularly useful in various scenarios, especially when a linear model is insufficient to capture the complexity of the data. Here are some common use cases:\n",
        "\n",
        "1. **Non-Linear Relationships:** When the relationship between the independent and dependent variables is not linear but can be better described by a curve. For instance, in cases where the data forms a parabolic shape.\n",
        "\n",
        "2. **Biological and Natural Processes:** Often used to model growth curves, dose-response curves, and other phenomena in biology and medicine where the relationship between variables is inherently non-linear.\n",
        "\n",
        "3. **Economics and Finance:** To model economic indicators and financial trends that exhibit cyclical or exponential growth patterns. For example, modeling the relationship between advertising expenditure and sales revenue.\n",
        "\n",
        "4. **Engineering and Physical Sciences:** To describe physical processes and systems where the relationship between variables follows a polynomial pattern, such as in signal processing or materials science.\n",
        "\n",
        "5. **Trend Analysis:** To fit smooth curves to time-series data and identify underlying trends. For example, to smooth noisy data in meteorology or climate studies.\n",
        "\n",
        "66. **Predictive Modeling:** When more complex models are required to improve predictive accuracy, polynomial regression can be used to capture non-linear patterns in the data.\n",
        "\n",
        "**In essence,** polynomial regression is a versatile tool for modeling and understanding complex relationships that cannot be adequately captured by simple linear regression. Its flexibility allows for better fitting and interpretation of data in various fields and applications."
      ],
      "metadata": {
        "id": "EI-JHkKgYkjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###26. What is the general equation for polynomial regression ?\n",
        "Ans-26. The general equation for polynomial regression of degree\n",
        "𝑛\n",
        " is:\n",
        "\n",
        "    𝑦 = 𝛽 0 + 𝛽 1 𝑥 + 𝛽 2 𝑥 2 + 𝛽 3 𝑥 3 + … + 𝛽 𝑛 𝑥 𝑛 + 𝜖\n",
        "\n",
        "Here's what each term represents:\n",
        "\n",
        "𝛽\n",
        "0\n",
        ": The intercept of the model.\n",
        "\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "𝛽\n",
        "3\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        ": The coefficients of the polynomial terms.\n",
        "\n",
        "𝑥\n",
        ": The independent variable.\n",
        "\n",
        "𝑦\n",
        ": The dependent variable.\n",
        "\n",
        "𝜖\n",
        ": The error term, representing the deviation of the observed values from the predicted values.\n",
        "\n",
        "##**Example for Degree 2 (Quadratic Regression)**\n",
        "\n",
        "For a quadratic regression (degree 2), the equation would be:\n",
        "\n",
        "    Y = 𝛽 0 + 𝛽 1 𝑥 + 𝛽 2 𝑥 2 + 𝜖\n",
        "\n",
        "##**Example for Degree 3 (Cubic Regression)**\n",
        "\n",
        "For a cubic regression (degree 3), the equation would be:\n",
        "\n",
        "    𝑦 = 𝛽 0 + 𝛽 1 𝑥 + 𝛽 2 𝑥 2 + 𝛽 3 𝑥 3 + 𝜖\n",
        "\n",
        "##**General Form**\n",
        "The general form can accommodate any degree\n",
        "𝑛\n",
        ", allowing the model to fit more complex, non-linear relationships between the variables.\n",
        "\n",
        "Polynomial regression is flexible and can capture the nuances in data that a linear model might miss. However, it also requires careful consideration of the polynomial degree to avoid overfitting.    \n",
        "\n"
      ],
      "metadata": {
        "id": "kAM0JVs0ZtNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###27. Can polynomial regression be applied to multiple variables ?\n",
        "Ans-27. **Yes, polynomial regression** can be applied to **multiple variables**, which is referred to as multivariate polynomial regression.\n",
        "\n",
        "####Here's how it works:\n",
        "\n",
        "1. **Multiple Input Variables:**\n",
        "\n",
        "* Instead of having a single input variable (x), you have multiple input variables (x1, x2, x3, ...).\n",
        "\n",
        "* These variables can be raised to different powers, creating interaction terms between them.\n",
        "\n",
        "2. **Polynomial Equation:**\n",
        "\n",
        "* The general form of a multivariate polynomial regression equation can be expressed as:\n",
        "\n",
        "    y = b0 + b1*x1 + b2*x2 + b3*x1^2 + b4*x1*x2 + b5*x2^2 + ... + higher-order terms + error\n",
        "\n",
        "* **y:** The dependent variable (the value you're trying to predict)\n",
        "\n",
        "* **x1, x2, ...:** The independent variables (the features)\n",
        "\n",
        "* **b0, b1, b2, ...:** The coefficients of the polynomial terms\n",
        "\n",
        "3. **Example:**\n",
        "\n",
        "* **Let's say you want to predict house prices based on multiple factors:**\n",
        "\n",
        "* x1: Size of the house (square feet)\n",
        "\n",
        "* x2: Number of bedrooms\n",
        "\n",
        "* x3: Age of the house\n",
        "\n",
        "* **A multivariate polynomial regression model might include terms like:**\n",
        "\n",
        "* x1^2 (size squared)\n",
        "\n",
        "* x1*x2 (interaction between size and number of bedrooms)\n",
        "\n",
        "* x2*x3 (interaction between number of bedrooms and age)\n",
        "\n",
        "4. **Flexibility and Complexity:**\n",
        "\n",
        "* Multivariate polynomial regression provides greater flexibility in modeling complex relationships between multiple input variables and the output.\n",
        "\n",
        "* However, it can also lead to overfitting if the degree of the polynomial is too high or if there are many interaction terms.\n",
        "\n",
        "5. **Implementation:**\n",
        "\n",
        "* Libraries like scikit-learn in Python provide tools to easily implement multivariate polynomial regression. You can use the PolynomialFeatures class to generate the polynomial features and then fit a linear regression model to the transformed data.\n",
        "\n",
        "####**Key Considerations:**\n",
        "\n",
        "* **Overfitting:** Be mindful of overfitting, especially with higher-degree polynomials. Techniques like cross-validation can help you choose the appropriate degree.\n",
        "\n",
        "* **Feature Engineering:** Careful feature engineering (creating new features from existing ones) can significantly improve the model's performance.\n",
        "\n",
        "* **Interpretability:** As the number of variables and interaction terms increases, interpreting the model can become more challenging.\n",
        "\n",
        "**In essence,** multivariate polynomial regression offers a powerful approach to modeling complex relationships in data with multiple predictors, but careful consideration must be given to potential overfitting and model interpretability."
      ],
      "metadata": {
        "id": "RgGbORC5czTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###28. What are the limitations of polynomial regression ?\n",
        "Ans-28. **Limitations of Polynomial Regression**\n",
        "\n",
        "Polynomial regression is a powerful tool for modeling non-linear relationships, but it has certain limitations:\n",
        "\n",
        "1. **Overfitting:**\n",
        "\n",
        "* High-degree polynomials can be prone to overfitting, especially with noisy data. This means the model fits the training data too closely, capturing noise and random fluctuations instead of the underlying trend.\n",
        "\n",
        "* This leads to poor generalization performance on new, unseen data.\n",
        "\n",
        "2. **Computational Complexity:**\n",
        "\n",
        "* Higher-degree polynomials involve more complex calculations, which can increase computational cost and slow down the training process, particularly with large datasets.\n",
        "\n",
        "3. **Selecting the Optimal Degree:**\n",
        "\n",
        "* Determining the appropriate degree for the polynomial can be challenging.\n",
        "\n",
        "* A low-degree polynomial may underfit the data (fail to capture the true relationship), while a high-degree polynomial may overfit.\n",
        "\n",
        "* Techniques like cross-validation are often needed to find the best degree, adding complexity to the modeling process.\n",
        "\n",
        "4. **Interpretability:**\n",
        "\n",
        "* Higher-degree polynomials can be difficult to interpret. The coefficients of the polynomial terms may not have straightforward interpretations.\n",
        "\n",
        "5. **Extrapolation:**\n",
        "\n",
        "* Polynomial regression models may not accurately predict values outside the range of the observed data, especially with high-degree polynomials.\n",
        "\n",
        "**In essence,** while polynomial regression offers flexibility in modeling non-linear relationships, careful consideration must be given to these limitations to avoid overfitting and ensure reliable predictions."
      ],
      "metadata": {
        "id": "hAeknPc3fVCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###29. What methods can be used to evaluate model fit when selecting the degree of a polynomial ?\n",
        "Ans-29. **Methods to Evaluate Model Fit when Selecting the Degree of a Polynomial.**\n",
        "\n",
        "When selecting the degree of a polynomial regression model, it's crucial to evaluate its fit to avoid overfitting or underfitting. Here are some common methods:\n",
        "\n",
        "1. **Visual Inspection:**\n",
        "\n",
        "**Plotting the Data and Fitted Curve:** Create a scatter plot of the data and plot the fitted polynomial curves for different degrees. Visually inspect how well the curve captures the trend in the data without being overly wiggly or too simplistic.\n",
        "\n",
        "**Residual Plots:** Plot the residuals (the difference between the actual values and the predicted values) against the fitted values or the independent variable. Ideally, the residuals should be randomly scattered around zero, indicating no systematic patterns.\n",
        "\n",
        "2. **Statistical Metrics:**\n",
        "\n",
        "* **R-squared (R²):** This measures the proportion of variance in the dependent variable explained by the model. Higher R² values generally indicate a better fit, but be cautious as R² tends to increase with higher degrees, even if the model is overfitting.\n",
        "\n",
        "* **Adjusted R-squared:** This modified version of R² penalizes the model for including unnecessary terms, providing a more balanced measure.\n",
        "\n",
        "* **Mean Squared Error (MSE):** This measures the average squared difference between the actual and predicted values. Lower MSE values indicate a better fit.\n",
        "\n",
        "* **Root Mean Squared Error (RMSE):** This is the square root of the MSE, providing a measure in the same units as the dependent variable.\n",
        "\n",
        "* **Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC):** These information criteria balance model fit with model complexity. Lower values generally indicate a better model.\n",
        "\n",
        "3. **Cross-Validation:**\n",
        "\n",
        "* **K-fold Cross-Validation:** Divide the data into k folds. Train the model on k-1 folds and evaluate its performance on the remaining fold. Repeat this process k times, using a different fold for evaluation each time. Calculate the average performance across all folds.\n",
        "\n",
        "* **Leave-One-Out Cross-Validation (LOOCV):** A special case of k-fold cross-validation where each fold consists of a single data point.\n",
        "\n",
        "4. **Regularization:**\n",
        "\n",
        "* **Ridge and Lasso Regression:** These methods add a penalty term to the model's loss function, discouraging large coefficients and helping to prevent overfitting. This can be particularly useful when dealing with high-degree polynomials.\n",
        "\n",
        "###**Choosing the Right Method:**\n",
        "\n",
        "The best method for evaluating model fit will depend on the specific dataset and the goals of the analysis. It's often helpful to use a combination of methods to get a comprehensive understanding of the model's performance.\n",
        "\n",
        "By carefully evaluating the model fit using these methods, you can select the appropriate degree for your polynomial regression model and ensure that it generalizes well to new data."
      ],
      "metadata": {
        "id": "WsZv0XMVhI8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###30. Why is visualization important in polynomial regression ?\n",
        "Ans-30 **Visualization is crucial in polynomial regression for several key reasons:**\n",
        "\n",
        "1. **Understanding Data Patterns:**\n",
        "\n",
        "* **Identifying Non-Linear Relationships:** Visualizing the data helps identify non-linear trends that linear regression might miss. A scatter plot can reveal curves, bends, or other patterns that suggest a polynomial relationship would be more appropriate.\n",
        "\n",
        "2. **Selecting the Optimal Degree:**\n",
        "\n",
        "* **Overfitting vs. Underfitting:** Plotting the data with polynomial curves of different degrees allows you to visually assess how well the model fits the data.\n",
        "\n",
        "* **Underfitting:** A low-degree polynomial might not capture the complexity of the data.\n",
        "\n",
        "* **Overfitting:** A high-degree polynomial might overreact to noise and create an overly complex curve that doesn't generalize well to new data.\n",
        "\n",
        "* **Visual Inspection:** By observing the curve's behavior, you can identify if it's too simple (underfitting) or too wiggly (overfitting).\n",
        "\n",
        "3. **Assessing Model Fit:**\n",
        "\n",
        "* **Residual Plots:** Plotting residuals (the difference between actual and predicted values) against the fitted values or the independent variable can help identify patterns. Ideally, residuals should be randomly scattered around zero. Systematic patterns in the residuals suggest potential problems with the model.\n",
        "\n",
        "4. **Communicating Results:**\n",
        "\n",
        "* **Clearer Explanations:** Visualizations like scatter plots with fitted curves make it much easier to communicate the model's findings to others. A picture can often convey complex relationships more effectively than just presenting numerical results.\n",
        "\n",
        "####**In summary, visualization is essential for:**\n",
        "\n",
        "* Data exploration\n",
        "\n",
        "* Model selection\n",
        "\n",
        "* Assessing model fit\n",
        "\n",
        "* Communicating findings\n",
        "\n",
        "By effectively using visualizations, you can gain a deeper understanding of your data, build better polynomial regression models, and effectively communicate your results."
      ],
      "metadata": {
        "id": "CbzCN7iOjFq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###31. How is polynomial regression implemented in Python?\n",
        "Ans-31. Implementing polynomial regression in Python is quite straightforward!\n",
        "\n",
        "####** Here's a step-by-step guide to get you started:**"
      ],
      "metadata": {
        "id": "lm790iW7kcrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "metadata": {
        "id": "Jcg7m1DJlZUr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create sample data\n",
        "\n",
        "# Create sample data with a quadratic relationship\n",
        "np.random.seed(0)\n",
        "x = np.linspace(-3, 3, 100)\n",
        "y = 0.8 * x**2 + 0.9 * x + 2 + np.random.randn(100)"
      ],
      "metadata": {
        "id": "zAN3MAnQlp0R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Split data into training and testing sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x.reshape(-1, 1), y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7RM-KOlJlusq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Create polynomial features\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)  # You can adjust the degree as needed\n",
        "x_train_poly = poly.fit_transform(x_train)\n",
        "x_test_poly = poly.transform(x_test)"
      ],
      "metadata": {
        "id": "PYSi7Xuul5XR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Fit the linear regression model\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(x_train_poly, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "jxP5hsIQmBw6",
        "outputId": "2d1e1e40-e42a-4030-8408-5e45c40e39c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Make predictions\n",
        "\n",
        "y_pred = model.predict(x_test_poly)"
      ],
      "metadata": {
        "id": "oFdjfNiQmNaq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluate the model\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVwHlLQxmYcu",
        "outputId": "330a919b-fbc2-4fb7-bfdd-e217c3a1cc49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.86\n",
            "R-squared: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Visualize the results\n",
        "\n",
        "plt.scatter(x_test, y_test, label='Actual')\n",
        "plt.scatter(x_test, y_pred, label='Predicted', color='red')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "WnnnGpz0mjC6",
        "outputId": "9ef07fe0-2d96-4316-aa65-213f8e721f1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7b7b7ef61910>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code demonstrates a basic implementation of polynomial regression in Python. we can adjust the degree of the polynomial using the degree parameter in Polynomial Features to find the best fit for our data."
      ],
      "metadata": {
        "id": "ECVSDU5Am5Hg"
      }
    }
  ]
}